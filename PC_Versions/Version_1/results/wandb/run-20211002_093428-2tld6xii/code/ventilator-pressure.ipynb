{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook  \n",
    "- PyTorch RNN starter code with W&B  \n",
    "- Pytorch W&B Usage Examples from https://docs.wandb.ai/guides/integrations/pytorch  \n",
    "\n",
    "If this notebook is helpful, feel free to upvote :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/google/deluca-lung/main/assets/2020-10-02%20Ventilator%20diagram.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    competition='ventilator'\n",
    "    _wandb_kernel='nakama'\n",
    "    apex=False\n",
    "    print_freq=100\n",
    "    num_workers=10\n",
    "    model_name='rnn'\n",
    "    scheduler='CosineAnnealingLR' # ['linear', 'cosine', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    batch_scheduler=False\n",
    "    #num_warmup_steps=100 # ['linear', 'cosine']\n",
    "    #num_cycles=0.5 # 'cosine'\n",
    "    #factor=0.2 # ReduceLROnPlateau\n",
    "    #patience=4 # ReduceLROnPlateau\n",
    "    #eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=50 # CosineAnnealingLR\n",
    "    #T_0=50 # CosineAnnealingWarmRestarts\n",
    "    epochs=5\n",
    "    max_grad_norm=1000\n",
    "    gradient_accumulation_steps=1\n",
    "    hidden_size=64\n",
    "    lr=7e-3\n",
    "    min_lr=2e-6\n",
    "    weight_decay=1e-6\n",
    "    batch_size=128\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    cate_seq_cols=['R', 'C']\n",
    "    cont_seq_cols=['time_step', 'u_in', 'u_out'] + ['breath_time', 'u_in_time']\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if CFG.apex:\n",
    "    from apex import amp\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \n",
      "Get your W&B access token from here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/anony-moose-140316/Ventilator-Pressure-Public?apiKey=2cddf237b244a7a18c9b07310860e6c2b2baea94\" target=\"_blank\">https://app.wandb.ai/anony-moose-140316/Ventilator-Pressure-Public?apiKey=2cddf237b244a7a18c9b07310860e6c2b2baea94</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/anony-moose-140316/Ventilator-Pressure-Public/runs/8sjutkj5?apiKey=2cddf237b244a7a18c9b07310860e6c2b2baea94\" target=\"_blank\">https://app.wandb.ai/anony-moose-140316/Ventilator-Pressure-Public/runs/8sjutkj5?apiKey=2cddf237b244a7a18c9b07310860e6c2b2baea94</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=secret_value_0)\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "    \n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "run = wandb.init(project=\"Ventilator-Pressure-Public\", \n",
    "                 name=CFG.model_name,\n",
    "                 config=class2dict(CFG),\n",
    "                 group=CFG.model_name,\n",
    "                 job_type=\"train\",\n",
    "                 anonymous=anony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_trues, y_preds):\n",
    "    score = mean_absolute_error(y_trues, y_preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080043</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>2.964399</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>3.157395</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.101542</td>\n",
       "      <td>3.170056</td>\n",
       "      <td>0</td>\n",
       "      <td>11.742872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>3.271690</td>\n",
       "      <td>0</td>\n",
       "      <td>12.234987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  R  C  time_step      u_in  u_out   pressure\n",
       "0   1          1  1  2   0.000000  0.080043      0   5.837492\n",
       "1   2          1  1  2   0.033652  2.964399      0   5.907794\n",
       "2   3          1  1  2   0.067514  3.157395      0   7.876254\n",
       "3   4          1  1  2   0.101542  3.170056      0  11.742872\n",
       "4   5          1  1  2   0.135756  3.271690      0  12.234987"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031904</td>\n",
       "      <td>2.141835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063827</td>\n",
       "      <td>2.750578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.095751</td>\n",
       "      <td>3.101470</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.127644</td>\n",
       "      <td>3.307654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  R  C  time_step      u_in  u_out\n",
       "0   1          0  0  1   0.000000  0.000000      0\n",
       "1   2          0  0  1   0.031904  2.141835      0\n",
       "2   3          0  0  1   0.063827  2.750578      0\n",
       "3   4          0  0  1   0.095751  3.101470      0\n",
       "4   5          0  0  1   0.127644  3.307654      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  pressure\n",
       "0   1         0\n",
       "1   2         0\n",
       "2   3         0\n",
       "3   4         0\n",
       "4   5         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "for c in ['u_in']:\n",
    "    train[c] = np.log1p(train[c])\n",
    "    test[c] = np.log1p(test[c])\n",
    "    \n",
    "r_map = {5: 0, 20: 1, 50: 2}\n",
    "c_map = {10: 0, 20: 1, 50: 2}\n",
    "train['R'] = train['R'].map(r_map)\n",
    "test['R'] = test['R'].map(r_map)\n",
    "train['C'] = train['C'].map(c_map)\n",
    "test['C'] = test['C'].map(c_map)\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# FE\n",
    "# ====================================================\n",
    "def add_feature(df):\n",
    "    # breath_time\n",
    "    df['breath_time'] = df['time_step'] - df['time_step'].shift(1)\n",
    "    df.loc[df['time_step'] == 0, 'breath_time'] = 0\n",
    "    # u_in_time\n",
    "    df['u_in_time'] = df['u_in'] - df['u_in'].shift(1)\n",
    "    df.loc[df['time_step'] == 0, 'u_in_time'] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "train = add_feature(train)\n",
    "test = add_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n",
      "0    1207200\n",
      "1    1207200\n",
      "2    1207200\n",
      "3    1207200\n",
      "4    1207200\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=5)\n",
    "groups = train['breath_id'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['pressure'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "print(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.hidden_size = cfg.hidden_size\n",
    "        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.seq_emb = nn.Sequential(\n",
    "            nn.Linear(4 + len(cfg.cont_seq_cols), self.hidden_size),\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, \n",
    "                            dropout=0.2, batch_first=True, bidirectional=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 2, self.hidden_size * 2),\n",
    "            nn.LayerNorm(self.hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.),\n",
    "            nn.Linear(self.hidden_size * 2, 1),\n",
    "        )\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                print(f'init {m}')\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        nn.init.normal_(param.data)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                print(f\"init {m}\")\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        init.normal_(param.data)\n",
    "\n",
    "    def forward(self, cate_seq_x, cont_seq_x):\n",
    "        bs = cont_seq_x.size(0)\n",
    "        r_emb = self.r_emb(cate_seq_x[:,:,0]).view(bs, 80, -1)\n",
    "        c_emb = self.c_emb(cate_seq_x[:,:,1]).view(bs, 80, -1)\n",
    "        seq_x = torch.cat((r_emb, c_emb, cont_seq_x), 2)\n",
    "        seq_emb = self.seq_emb(seq_x)\n",
    "        seq_emb, _ = self.lstm(seq_emb)\n",
    "        output = self.head(seq_emb).view(bs, -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# helper function\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    for step, (cate_seq_x, cont_seq_x, u_out, y) in enumerate(train_loader):\n",
    "        loss_mask = u_out == 0\n",
    "        cate_seq_x, cont_seq_x, y = cate_seq_x.to(device), cont_seq_x.to(device), y.to(device)\n",
    "        batch_size = cont_seq_x.size(0)\n",
    "        pred = model(cate_seq_x, cont_seq_x)\n",
    "        loss = 2. * criterion(pred[loss_mask], y[loss_mask]) + criterion(pred[loss_mask == 0], y[loss_mask == 0])\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        if CFG.apex:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader),\n",
    "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   loss=losses,\n",
    "                   grad_norm=grad_norm,\n",
    "                   lr=scheduler.get_lr()[0],\n",
    "                   ))\n",
    "        wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                   f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    for step, (cate_seq_x, cont_seq_x, u_out, y) in enumerate(valid_loader):\n",
    "        loss_mask = u_out == 0\n",
    "        cate_seq_x, cont_seq_x, y = cate_seq_x.to(device), cont_seq_x.to(device), y.to(device)\n",
    "        batch_size = cont_seq_x.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(cate_seq_x, cont_seq_x)\n",
    "        loss = 2. * criterion(pred[loss_mask], y[loss_mask]) + criterion(pred[loss_mask == 0], y[loss_mask == 0])\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(pred.view(-1).detach().cpu().numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader),\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   loss=losses,\n",
    "                   ))\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, (cate_seq_x, cont_seq_x) in tk0:\n",
    "        cate_seq_x, cont_seq_x = cate_seq_x.to(device), cont_seq_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(cate_seq_x, cont_seq_x)\n",
    "        preds.append(pred.view(-1).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "    \n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "    y_true = valid_folds['pressure'].values\n",
    "    non_expiratory_phase_val_idx = valid_folds[valid_folds['u_out'] == 0].index # The expiratory phase is not scored\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds)\n",
    "    valid_dataset = TrainDataset(valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG)\n",
    "    model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    \n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif CFG.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=CFG.num_cycles\n",
    "            )\n",
    "        elif CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # apex\n",
    "    # ====================================================\n",
    "    if CFG.apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(y_true[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - MAE Score (without expiratory phase): {score:.4f}')\n",
    "        wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                   f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                   f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                   f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'preds': preds},\n",
    "                        OUTPUT_DIR+f\"fold{fold}_best.pth\")\n",
    "            \n",
    "    preds = torch.load(OUTPUT_DIR+f\"fold{fold}_best.pth\", map_location=torch.device('cpu'))['preds']\n",
    "    valid_folds['preds'] = preds\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare: 1.train 2.test\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_result(result_df):\n",
    "        preds = result_df['preds'].values\n",
    "        labels = result_df['pressure'].values\n",
    "        non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n",
    "        score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n",
    "        LOGGER.info(f'Score (without expiratory phase): {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        # CV result\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        # save result\n",
    "        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n",
    "    \n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n",
    "        for fold in CFG.trn_fold:\n",
    "            model = CustomModel(CFG)\n",
    "            path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n",
    "            state = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "            predictions = inference_fn(test_loader, model, device)\n",
    "            test[f'fold{fold}'] = predictions\n",
    "            del state, predictions; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        # submission\n",
    "        test['pressure'] = test[[f'fold{fold}' for fold in range(CFG.n_fold)]].mean(1)\n",
    "        test[['id', 'pressure']+[f'fold{fold}' for fold in range(CFG.n_fold)]].to_csv(OUTPUT_DIR+'raw_submission.csv', index=False)\n",
    "        test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission.csv', index=False)\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 18s (remain 618m 6s) Loss: 43.9482(43.9482) Grad: 33.1743  LR: 0.007000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][100/471] Elapsed 1m 25s (remain 5m 11s) Loss: 7.1452(17.1981) Grad: 34.8585  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 30s (remain 2m 1s) Loss: 4.7035(11.3927) Grad: 21.8319  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 36s (remain 0m 54s) Loss: 4.2165(9.0646) Grad: 17.9261  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 42s (remain 0m 17s) Loss: 3.2255(7.7712) Grad: 8.8629  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 45s (remain 0m 0s) Loss: 3.9639(7.1688) Grad: 28.8656  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 126m 20s) Loss: 3.4464(3.4464) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 3.3504(3.3635) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 2.9914(3.3677) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 7.1688  avg_val_loss: 3.3677  time: 181s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4556\n",
      "Epoch 1 - Save Best Score: 1.4556 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 22s (remain 643m 54s) Loss: 3.4560(3.4560) Grad: 21.0815  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 28s (remain 5m 24s) Loss: 3.4363(3.5389) Grad: 13.7987  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 34s (remain 2m 7s) Loss: 3.5079(3.5383) Grad: 31.3212  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 40s (remain 0m 56s) Loss: 3.1888(3.4652) Grad: 9.9116  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 46s (remain 0m 18s) Loss: 3.3274(3.4050) Grad: 30.7225  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 50s (remain 0m 0s) Loss: 3.0428(3.3616) Grad: 13.5117  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 13s (remain 142m 49s) Loss: 2.9982(2.9982) \n",
      "EVAL: [100/118] Elapsed 1m 18s (remain 0m 13s) Loss: 3.0492(2.7913) \n",
      "EVAL: [117/118] Elapsed 1m 19s (remain 0m 0s) Loss: 2.5823(2.7907) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.3616  avg_val_loss: 2.7907  time: 194s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1899\n",
      "Epoch 2 - Save Best Score: 1.1899 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 14s (remain 583m 20s) Loss: 2.8067(2.8067) Grad: 30.1352  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 20s (remain 4m 56s) Loss: 3.0913(3.0298) Grad: 33.9149  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 26s (remain 1m 55s) Loss: 2.6841(2.9985) Grad: 7.9879  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 31s (remain 0m 51s) Loss: 2.7739(3.0124) Grad: 8.2621  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 37s (remain 0m 16s) Loss: 2.9751(3.0194) Grad: 8.2712  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 40s (remain 0m 0s) Loss: 2.8913(3.0345) Grad: 21.9004  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 6s (remain 128m 43s) Loss: 3.0609(3.0609) \n",
      "EVAL: [100/118] Elapsed 1m 11s (remain 0m 11s) Loss: 2.9809(2.8697) \n",
      "EVAL: [117/118] Elapsed 1m 11s (remain 0m 0s) Loss: 2.5261(2.8705) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 3.0345  avg_val_loss: 2.8705  time: 177s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.2084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 20s (remain 628m 23s) Loss: 2.8938(2.8938) Grad: 24.5020  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 27s (remain 5m 20s) Loss: 2.6947(2.8619) Grad: 13.2794  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 33s (remain 2m 5s) Loss: 2.7713(2.8110) Grad: 18.6631  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 39s (remain 0m 56s) Loss: 2.6939(2.8235) Grad: 25.2091  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 45s (remain 0m 18s) Loss: 2.6141(2.7991) Grad: 10.0066  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 49s (remain 0m 0s) Loss: 2.8037(2.7918) Grad: 11.9395  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 12s (remain 140m 52s) Loss: 2.7394(2.7394) \n",
      "EVAL: [100/118] Elapsed 1m 18s (remain 0m 13s) Loss: 2.6210(2.5354) \n",
      "EVAL: [117/118] Elapsed 1m 18s (remain 0m 0s) Loss: 2.4511(2.5342) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.7918  avg_val_loss: 2.5342  time: 193s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0643\n",
      "Epoch 4 - Save Best Score: 1.0643 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 21s (remain 638m 11s) Loss: 2.7277(2.7277) Grad: 15.6795  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 29s (remain 5m 29s) Loss: 2.5696(2.6687) Grad: 35.2863  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 36s (remain 2m 9s) Loss: 2.9683(2.7015) Grad: 32.6032  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 42s (remain 0m 57s) Loss: 2.5496(2.6863) Grad: 11.3816  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 48s (remain 0m 18s) Loss: 2.6287(2.6750) Grad: 26.4001  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 52s (remain 0m 0s) Loss: 2.5892(2.6552) Grad: 6.5931  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 9s (remain 135m 6s) Loss: 2.6648(2.6648) \n",
      "EVAL: [100/118] Elapsed 1m 14s (remain 0m 12s) Loss: 2.4816(2.4331) \n",
      "EVAL: [117/118] Elapsed 1m 15s (remain 0m 0s) Loss: 2.2661(2.4334) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.6552  avg_val_loss: 2.4334  time: 193s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 1.0254\n",
      "Epoch 5 - Save Best Score: 1.0254 Model\n",
      "========== fold: 0 result ==========\n",
      "Score (without expiratory phase): 1.0254\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 21s (remain 640m 21s) Loss: 42.3075(42.3075) Grad: 34.9289  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 27s (remain 5m 21s) Loss: 7.3215(16.5868) Grad: 14.4522  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 34s (remain 2m 6s) Loss: 4.8775(11.0917) Grad: 28.1245  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 41s (remain 0m 57s) Loss: 3.8520(8.8559) Grad: 7.3763  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 47s (remain 0m 18s) Loss: 4.1754(7.6331) Grad: 25.3358  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 51s (remain 0m 0s) Loss: 3.2622(7.0402) Grad: 8.7968  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 10s (remain 137m 47s) Loss: 3.4157(3.4157) \n",
      "EVAL: [100/118] Elapsed 1m 15s (remain 0m 12s) Loss: 3.0385(3.2288) \n",
      "EVAL: [117/118] Elapsed 1m 15s (remain 0m 0s) Loss: 2.8021(3.2213) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 7.0402  avg_val_loss: 3.2213  time: 192s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.3664\n",
      "Epoch 1 - Save Best Score: 1.3664 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 14s (remain 581m 43s) Loss: 4.0504(4.0504) Grad: 24.8217  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 20s (remain 4m 53s) Loss: 3.4943(3.4029) Grad: 40.6447  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 26s (remain 1m 55s) Loss: 3.4975(3.3385) Grad: 30.7221  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 32s (remain 0m 52s) Loss: 2.7011(3.2849) Grad: 12.1150  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 38s (remain 0m 17s) Loss: 3.0692(3.2642) Grad: 17.1192  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 42s (remain 0m 0s) Loss: 2.9057(3.2213) Grad: 12.9508  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 6s (remain 129m 34s) Loss: 2.7243(2.7243) \n",
      "EVAL: [100/118] Elapsed 1m 11s (remain 0m 12s) Loss: 2.4600(2.6738) \n",
      "EVAL: [117/118] Elapsed 1m 12s (remain 0m 0s) Loss: 2.3231(2.6699) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.2213  avg_val_loss: 2.6699  time: 179s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1320\n",
      "Epoch 2 - Save Best Score: 1.1320 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 15s (remain 590m 40s) Loss: 2.7161(2.7161) Grad: 20.7176  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 21s (remain 4m 56s) Loss: 2.6197(2.9794) Grad: 13.0518  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 26s (remain 1m 56s) Loss: 2.8501(2.9641) Grad: 12.9295  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 32s (remain 0m 52s) Loss: 2.9747(2.9381) Grad: 20.8635  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 38s (remain 0m 17s) Loss: 2.9405(2.9292) Grad: 8.4314  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 42s (remain 0m 0s) Loss: 2.7923(2.9008) Grad: 9.9474  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 3s (remain 124m 44s) Loss: 2.4900(2.4900) \n",
      "EVAL: [100/118] Elapsed 1m 8s (remain 0m 11s) Loss: 2.3779(2.4955) \n",
      "EVAL: [117/118] Elapsed 1m 9s (remain 0m 0s) Loss: 2.1729(2.4923) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.9008  avg_val_loss: 2.4923  time: 177s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0463\n",
      "Epoch 3 - Save Best Score: 1.0463 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 15s (remain 593m 10s) Loss: 2.4497(2.4497) Grad: 16.7926  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 21s (remain 4m 59s) Loss: 2.7272(2.8002) Grad: 26.1541  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 27s (remain 1m 57s) Loss: 2.7930(2.7664) Grad: 36.8916  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 33s (remain 0m 52s) Loss: 2.7358(2.7489) Grad: 21.9848  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 39s (remain 0m 17s) Loss: 2.6104(2.7256) Grad: 10.2862  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 42s (remain 0m 0s) Loss: 2.8063(2.7169) Grad: 34.8477  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 35s) Loss: 2.4196(2.4196) \n",
      "EVAL: [100/118] Elapsed 1m 11s (remain 0m 12s) Loss: 2.2307(2.4258) \n",
      "EVAL: [117/118] Elapsed 1m 11s (remain 0m 0s) Loss: 2.0250(2.4191) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.7169  avg_val_loss: 2.4191  time: 180s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0193\n",
      "Epoch 4 - Save Best Score: 1.0193 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 15s (remain 591m 7s) Loss: 2.6127(2.6127) Grad: 19.0742  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 21s (remain 4m 57s) Loss: 2.2680(2.6591) Grad: 13.5311  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 26s (remain 1m 56s) Loss: 2.5903(2.6132) Grad: 8.8325  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 32s (remain 0m 51s) Loss: 2.3863(2.6044) Grad: 16.2703  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 37s (remain 0m 17s) Loss: 2.5896(2.5880) Grad: 22.4251  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 41s (remain 0m 0s) Loss: 2.5346(2.5737) Grad: 26.6059  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 126m 39s) Loss: 2.1697(2.1697) \n",
      "EVAL: [100/118] Elapsed 1m 9s (remain 0m 11s) Loss: 2.1247(2.2791) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 1.9538(2.2738) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5737  avg_val_loss: 2.2738  time: 176s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9558\n",
      "Epoch 5 - Save Best Score: 0.9558 Model\n",
      "========== fold: 1 result ==========\n",
      "Score (without expiratory phase): 0.9558\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 25s (remain 672m 8s) Loss: 40.6192(40.6192) Grad: 33.1393  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 31s (remain 5m 36s) Loss: 6.3876(15.3596) Grad: 12.2307  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 38s (remain 2m 11s) Loss: 3.9290(10.3068) Grad: 11.1202  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 43s (remain 0m 58s) Loss: 3.9597(8.2999) Grad: 35.3820  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 49s (remain 0m 19s) Loss: 3.4495(7.1845) Grad: 11.2621  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 54s (remain 0m 0s) Loss: 3.6835(6.6762) Grad: 30.5764  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 22s) Loss: 3.5690(3.5690) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 3.1302(3.1606) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 2.7433(3.1733) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.6762  avg_val_loss: 3.1733  time: 189s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.3608\n",
      "Epoch 1 - Save Best Score: 1.3608 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 16s (remain 597m 27s) Loss: 3.8031(3.8031) Grad: 13.5622  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 22s (remain 5m 1s) Loss: 3.0871(3.4470) Grad: 9.2890  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 28s (remain 1m 58s) Loss: 2.9369(3.3554) Grad: 21.8494  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 2.9558(3.3235) Grad: 33.9224  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 40s (remain 0m 17s) Loss: 2.7424(3.2793) Grad: 15.6413  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 45s (remain 0m 0s) Loss: 3.1002(3.2512) Grad: 27.5949  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 126m 32s) Loss: 2.8601(2.8601) \n",
      "EVAL: [100/118] Elapsed 1m 9s (remain 0m 11s) Loss: 2.5723(2.5990) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 2.3944(2.6145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.2512  avg_val_loss: 2.6145  time: 180s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1052\n",
      "Epoch 2 - Save Best Score: 1.1052 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 18s (remain 612m 52s) Loss: 2.5772(2.5772) Grad: 9.2159  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 24s (remain 5m 8s) Loss: 3.1133(2.9811) Grad: 43.1716  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 29s (remain 2m 0s) Loss: 2.7517(2.9668) Grad: 11.2723  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 35s (remain 0m 54s) Loss: 2.5462(2.9413) Grad: 8.8519  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 41s (remain 0m 17s) Loss: 2.7414(2.9171) Grad: 31.7580  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 44s (remain 0m 0s) Loss: 2.7790(2.9082) Grad: 28.9277  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 125m 35s) Loss: 3.0035(3.0035) \n",
      "EVAL: [100/118] Elapsed 1m 9s (remain 0m 11s) Loss: 2.9293(2.8587) \n",
      "EVAL: [117/118] Elapsed 1m 9s (remain 0m 0s) Loss: 2.5413(2.8732) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.9082  avg_val_loss: 2.8732  time: 179s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.2183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 15s (remain 589m 51s) Loss: 2.5140(2.5140) Grad: 21.1561  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 21s (remain 4m 58s) Loss: 2.7212(2.8602) Grad: 9.5227  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 27s (remain 1m 57s) Loss: 2.6874(2.7757) Grad: 10.8420  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 33s (remain 0m 52s) Loss: 2.9209(2.7572) Grad: 14.5978  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 39s (remain 0m 17s) Loss: 2.4225(2.7344) Grad: 11.8598  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 42s (remain 0m 0s) Loss: 2.4400(2.7144) Grad: 7.6521  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 125m 1s) Loss: 3.0470(3.0470) \n",
      "EVAL: [100/118] Elapsed 1m 8s (remain 0m 11s) Loss: 2.8657(2.7695) \n",
      "EVAL: [117/118] Elapsed 1m 9s (remain 0m 0s) Loss: 2.4242(2.7907) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.7144  avg_val_loss: 2.7907  time: 177s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 14s (remain 587m 15s) Loss: 2.7410(2.7410) Grad: 36.5032  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 20s (remain 4m 55s) Loss: 2.7272(2.6909) Grad: 19.1188  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 26s (remain 1m 56s) Loss: 2.9155(2.6718) Grad: 32.7524  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 32s (remain 0m 52s) Loss: 2.4949(2.6500) Grad: 16.6389  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 38s (remain 0m 17s) Loss: 2.2790(2.6188) Grad: 7.2778  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 41s (remain 0m 0s) Loss: 2.7563(2.6300) Grad: 9.9694  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 40s) Loss: 2.7033(2.7033) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 2.6309(2.5867) \n",
      "EVAL: [117/118] Elapsed 1m 11s (remain 0m 0s) Loss: 2.3845(2.6027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.6300  avg_val_loss: 2.6027  time: 177s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 1.0974\n",
      "Epoch 5 - Save Best Score: 1.0974 Model\n",
      "========== fold: 2 result ==========\n",
      "Score (without expiratory phase): 1.0974\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 19s (remain 619m 23s) Loss: 43.6285(43.6285) Grad: 32.3233  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 25s (remain 5m 12s) Loss: 7.3797(17.1268) Grad: 18.1614  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 31s (remain 2m 2s) Loss: 4.9195(11.5034) Grad: 16.8614  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 37s (remain 0m 55s) Loss: 3.8871(9.1402) Grad: 34.2732  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 44s (remain 0m 18s) Loss: 4.4792(7.8406) Grad: 42.3762  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 48s (remain 0m 0s) Loss: 3.4006(7.2172) Grad: 17.3107  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 11s (remain 139m 54s) Loss: 3.4452(3.4452) \n",
      "EVAL: [100/118] Elapsed 1m 17s (remain 0m 12s) Loss: 2.7983(3.0928) \n",
      "EVAL: [117/118] Elapsed 1m 17s (remain 0m 0s) Loss: 3.2414(3.0809) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 7.2172  avg_val_loss: 3.0809  time: 191s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.3185\n",
      "Epoch 1 - Save Best Score: 1.3185 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 22s (remain 646m 52s) Loss: 3.3269(3.3269) Grad: 11.1390  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 30s (remain 5m 31s) Loss: 3.8717(3.4305) Grad: 21.1279  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 37s (remain 2m 10s) Loss: 3.5256(3.3619) Grad: 46.9491  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 43s (remain 0m 58s) Loss: 3.3824(3.3154) Grad: 41.8820  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 50s (remain 0m 19s) Loss: 3.0889(3.2549) Grad: 28.8349  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 55s (remain 0m 0s) Loss: 2.9015(3.2133) Grad: 16.2227  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 8s (remain 133m 45s) Loss: 3.0410(3.0410) \n",
      "EVAL: [100/118] Elapsed 1m 13s (remain 0m 12s) Loss: 2.5736(2.9078) \n",
      "EVAL: [117/118] Elapsed 1m 14s (remain 0m 0s) Loss: 3.0591(2.9017) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.2133  avg_val_loss: 2.9017  time: 195s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.2197\n",
      "Epoch 2 - Save Best Score: 1.2197 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 20s (remain 627m 33s) Loss: 2.9544(2.9544) Grad: 23.0521  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 26s (remain 5m 16s) Loss: 3.0279(3.0759) Grad: 20.1752  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 32s (remain 2m 4s) Loss: 3.2686(3.0008) Grad: 20.7788  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 38s (remain 0m 55s) Loss: 2.5185(2.9694) Grad: 9.0775  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 44s (remain 0m 18s) Loss: 2.7093(2.9326) Grad: 24.9299  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 48s (remain 0m 0s) Loss: 2.8423(2.9207) Grad: 12.6221  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 7s (remain 131m 27s) Loss: 2.7331(2.7331) \n",
      "EVAL: [100/118] Elapsed 1m 12s (remain 0m 12s) Loss: 2.5030(2.5391) \n",
      "EVAL: [117/118] Elapsed 1m 13s (remain 0m 0s) Loss: 2.7063(2.5291) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.9207  avg_val_loss: 2.5291  time: 187s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0736\n",
      "Epoch 3 - Save Best Score: 1.0736 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 19s (remain 619m 43s) Loss: 2.8471(2.8471) Grad: 23.9475  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 24s (remain 5m 11s) Loss: 3.0654(2.7587) Grad: 14.2618  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 30s (remain 2m 1s) Loss: 2.8466(2.7435) Grad: 21.8671  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 36s (remain 0m 54s) Loss: 2.4473(2.7251) Grad: 10.8030  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 42s (remain 0m 17s) Loss: 2.2978(2.7031) Grad: 16.3683  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 46s (remain 0m 0s) Loss: 2.8247(2.6939) Grad: 10.8157  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 6s (remain 129m 29s) Loss: 2.7023(2.7023) \n",
      "EVAL: [100/118] Elapsed 1m 11s (remain 0m 12s) Loss: 2.4665(2.4606) \n",
      "EVAL: [117/118] Elapsed 1m 11s (remain 0m 0s) Loss: 2.3960(2.4545) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.6939  avg_val_loss: 2.4545  time: 183s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0336\n",
      "Epoch 4 - Save Best Score: 1.0336 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 16s (remain 599m 50s) Loss: 2.6759(2.6759) Grad: 28.8396  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 23s (remain 5m 4s) Loss: 2.7960(2.5609) Grad: 23.4943  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 29s (remain 1m 59s) Loss: 2.6895(2.5964) Grad: 12.4819  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 2.4249(2.5713) Grad: 6.2350  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 40s (remain 0m 17s) Loss: 2.5241(2.5546) Grad: 13.9017  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 44s (remain 0m 0s) Loss: 2.4376(2.5494) Grad: 6.4899  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 0s) Loss: 2.6805(2.6805) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 2.5139(2.5094) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 2.3678(2.5019) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5494  avg_val_loss: 2.5019  time: 180s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 1.0689\n",
      "========== fold: 3 result ==========\n",
      "Score (without expiratory phase): 1.0336\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 18s (remain 613m 26s) Loss: 41.7337(41.7337) Grad: 34.9991  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 24s (remain 5m 8s) Loss: 5.8364(15.7420) Grad: 7.9733  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 30s (remain 2m 1s) Loss: 3.7189(10.4511) Grad: 7.4126  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 36s (remain 0m 54s) Loss: 3.8317(8.3541) Grad: 15.6698  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 42s (remain 0m 17s) Loss: 3.9999(7.1984) Grad: 22.6697  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 46s (remain 0m 0s) Loss: 3.8001(6.6509) Grad: 33.3210  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 19s) Loss: 3.3620(3.3620) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 3.6569(3.4134) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 3.2541(3.4210) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.6509  avg_val_loss: 3.4210  time: 182s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4789\n",
      "Epoch 1 - Save Best Score: 1.4789 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 16s (remain 599m 50s) Loss: 3.4920(3.4920) Grad: 20.2931  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 22s (remain 5m 2s) Loss: 2.9341(3.3993) Grad: 10.0034  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 28s (remain 1m 59s) Loss: 3.0072(3.3554) Grad: 22.9095  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 3.1746(3.2878) Grad: 38.4475  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 40s (remain 0m 17s) Loss: 4.3024(3.2397) Grad: 48.3180  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 43s (remain 0m 0s) Loss: 2.7724(3.2353) Grad: 10.5737  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 23s) Loss: 2.8105(2.8105) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 3.0631(2.8192) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 2.9316(2.8259) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.2353  avg_val_loss: 2.8259  time: 180s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.2016\n",
      "Epoch 2 - Save Best Score: 1.2016 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 16s (remain 595m 46s) Loss: 2.9320(2.9320) Grad: 17.6254  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 22s (remain 5m 0s) Loss: 2.8048(2.9677) Grad: 17.3059  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 27s (remain 1m 58s) Loss: 3.0490(3.0673) Grad: 21.8369  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 2.9917(3.0131) Grad: 30.1998  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 40s (remain 0m 17s) Loss: 2.9567(3.0102) Grad: 10.7547  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 44s (remain 0m 0s) Loss: 2.8650(2.9827) Grad: 37.4514  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 6s (remain 129m 38s) Loss: 2.4862(2.4862) \n",
      "EVAL: [100/118] Elapsed 1m 11s (remain 0m 12s) Loss: 2.8632(2.5511) \n",
      "EVAL: [117/118] Elapsed 1m 11s (remain 0m 0s) Loss: 2.5619(2.5604) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.9827  avg_val_loss: 2.5604  time: 181s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0736\n",
      "Epoch 3 - Save Best Score: 1.0736 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 16s (remain 596m 1s) Loss: 2.5939(2.5939) Grad: 16.7220  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 22s (remain 5m 1s) Loss: 2.5987(2.7533) Grad: 32.9079  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 28s (remain 1m 58s) Loss: 3.0155(2.7403) Grad: 15.9722  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 2.6209(2.7124) Grad: 31.3802  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 40s (remain 0m 17s) Loss: 2.2121(2.7070) Grad: 19.7233  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 43s (remain 0m 0s) Loss: 2.5769(2.6941) Grad: 14.8334  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 3s (remain 123m 22s) Loss: 2.4420(2.4420) \n",
      "EVAL: [100/118] Elapsed 1m 7s (remain 0m 11s) Loss: 2.4352(2.3741) \n",
      "EVAL: [117/118] Elapsed 1m 8s (remain 0m 0s) Loss: 2.3445(2.3770) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.6941  avg_val_loss: 2.3770  time: 177s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 0.9949\n",
      "Epoch 4 - Save Best Score: 0.9949 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 12s (remain 569m 7s) Loss: 2.5441(2.5441) Grad: 10.6185  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 18s (remain 4m 47s) Loss: 2.4027(2.5965) Grad: 19.4382  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 23s (remain 1m 52s) Loss: 2.6628(2.5658) Grad: 9.7372  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 29s (remain 0m 50s) Loss: 2.5585(2.5714) Grad: 35.0139  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 34s (remain 0m 16s) Loss: 2.9794(2.5802) Grad: 33.5342  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 38s (remain 0m 0s) Loss: 2.5819(2.5741) Grad: 12.2258  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 2s (remain 121m 15s) Loss: 2.3478(2.3478) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 2.5558(2.3487) \n",
      "EVAL: [117/118] Elapsed 1m 7s (remain 0m 0s) Loss: 2.2468(2.3533) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5741  avg_val_loss: 2.3533  time: 170s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9832\n",
      "Epoch 5 - Save Best Score: 0.9832 Model\n",
      "========== fold: 4 result ==========\n",
      "Score (without expiratory phase): 0.9832\n",
      "========== CV ==========\n",
      "Score (without expiratory phase): 1.0234\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TestDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-690d7c1996d9>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTestDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrn_fold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TestDataset' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
