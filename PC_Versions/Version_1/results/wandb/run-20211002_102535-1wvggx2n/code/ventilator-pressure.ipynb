{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook  \n",
    "- PyTorch RNN starter code with W&B  \n",
    "- Pytorch W&B Usage Examples from https://docs.wandb.ai/guides/integrations/pytorch  \n",
    "\n",
    "If this notebook is helpful, feel free to upvote :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/google/deluca-lung/main/assets/2020-10-02%20Ventilator%20diagram.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    competition='ventilator'\n",
    "    _wandb_kernel='nakama'\n",
    "    apex=False\n",
    "    print_freq=100\n",
    "    num_workers=10\n",
    "    model_name='rnn'\n",
    "    scheduler='CosineAnnealingLR' # ['linear', 'cosine', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    batch_scheduler=False\n",
    "    #num_warmup_steps=100 # ['linear', 'cosine']\n",
    "    #num_cycles=0.5 # 'cosine'\n",
    "    #factor=0.2 # ReduceLROnPlateau\n",
    "    #patience=4 # ReduceLROnPlateau\n",
    "    #eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=50 # CosineAnnealingLR\n",
    "    #T_0=50 # CosineAnnealingWarmRestarts\n",
    "    epochs=6\n",
    "    max_grad_norm=1000\n",
    "    gradient_accumulation_steps=1\n",
    "    hidden_size=64\n",
    "    lr=7e-3\n",
    "    min_lr=2e-6\n",
    "    weight_decay=1e-6\n",
    "    batch_size=128\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    cate_seq_cols=['R', 'C']\n",
    "    cont_seq_cols=['time_step', 'u_in', 'u_out'] + ['breath_time', 'u_in_time']\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if CFG.apex:\n",
    "    from apex import amp\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \n",
      "Get your W&B access token from here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/anony-moose-140316/Ventilator-Pressure-Public?apiKey=2cddf237b244a7a18c9b07310860e6c2b2baea94\" target=\"_blank\">https://app.wandb.ai/anony-moose-140316/Ventilator-Pressure-Public?apiKey=2cddf237b244a7a18c9b07310860e6c2b2baea94</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/anony-moose-140316/Ventilator-Pressure-Public/runs/2tld6xii?apiKey=2cddf237b244a7a18c9b07310860e6c2b2baea94\" target=\"_blank\">https://app.wandb.ai/anony-moose-140316/Ventilator-Pressure-Public/runs/2tld6xii?apiKey=2cddf237b244a7a18c9b07310860e6c2b2baea94</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=secret_value_0)\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "    \n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "run = wandb.init(project=\"Ventilator-Pressure-Public\", \n",
    "                 name=CFG.model_name,\n",
    "                 config=class2dict(CFG),\n",
    "                 group=CFG.model_name,\n",
    "                 job_type=\"train\",\n",
    "                 anonymous=anony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_trues, y_preds):\n",
    "    score = mean_absolute_error(y_trues, y_preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080043</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>2.964399</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>3.157395</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.101542</td>\n",
       "      <td>3.170056</td>\n",
       "      <td>0</td>\n",
       "      <td>11.742872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>3.271690</td>\n",
       "      <td>0</td>\n",
       "      <td>12.234987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  R  C  time_step      u_in  u_out   pressure\n",
       "0   1          1  1  2   0.000000  0.080043      0   5.837492\n",
       "1   2          1  1  2   0.033652  2.964399      0   5.907794\n",
       "2   3          1  1  2   0.067514  3.157395      0   7.876254\n",
       "3   4          1  1  2   0.101542  3.170056      0  11.742872\n",
       "4   5          1  1  2   0.135756  3.271690      0  12.234987"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031904</td>\n",
       "      <td>2.141835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063827</td>\n",
       "      <td>2.750578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.095751</td>\n",
       "      <td>3.101470</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.127644</td>\n",
       "      <td>3.307654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  R  C  time_step      u_in  u_out\n",
       "0   1          0  0  1   0.000000  0.000000      0\n",
       "1   2          0  0  1   0.031904  2.141835      0\n",
       "2   3          0  0  1   0.063827  2.750578      0\n",
       "3   4          0  0  1   0.095751  3.101470      0\n",
       "4   5          0  0  1   0.127644  3.307654      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  pressure\n",
       "0   1         0\n",
       "1   2         0\n",
       "2   3         0\n",
       "3   4         0\n",
       "4   5         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "for c in ['u_in']:\n",
    "    train[c] = np.log1p(train[c])\n",
    "    test[c] = np.log1p(test[c])\n",
    "    \n",
    "r_map = {5: 0, 20: 1, 50: 2}\n",
    "c_map = {10: 0, 20: 1, 50: 2}\n",
    "train['R'] = train['R'].map(r_map)\n",
    "test['R'] = test['R'].map(r_map)\n",
    "train['C'] = train['C'].map(c_map)\n",
    "test['C'] = test['C'].map(c_map)\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# FE\n",
    "# ====================================================\n",
    "def add_feature(df):\n",
    "    # breath_time\n",
    "    df['breath_time'] = df['time_step'] - df['time_step'].shift(1)\n",
    "    df.loc[df['time_step'] == 0, 'breath_time'] = 0\n",
    "    # u_in_time\n",
    "    df['u_in_time'] = df['u_in'] - df['u_in'].shift(1)\n",
    "    df.loc[df['time_step'] == 0, 'u_in_time'] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "train = add_feature(train)\n",
    "test = add_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n",
      "0    1207200\n",
      "1    1207200\n",
      "2    1207200\n",
      "3    1207200\n",
      "4    1207200\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=5)\n",
    "groups = train['breath_id'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['pressure'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "print(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.hidden_size = cfg.hidden_size\n",
    "        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.seq_emb = nn.Sequential(\n",
    "            nn.Linear(4 + len(cfg.cont_seq_cols), self.hidden_size),\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, \n",
    "                            dropout=0.2, batch_first=True, bidirectional=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 2, self.hidden_size * 2),\n",
    "            nn.LayerNorm(self.hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.),\n",
    "            nn.Linear(self.hidden_size * 2, 1),\n",
    "        )\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                print(f'init {m}')\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        nn.init.normal_(param.data)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                print(f\"init {m}\")\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        init.normal_(param.data)\n",
    "\n",
    "    def forward(self, cate_seq_x, cont_seq_x):\n",
    "        bs = cont_seq_x.size(0)\n",
    "        r_emb = self.r_emb(cate_seq_x[:,:,0]).view(bs, 80, -1)\n",
    "        c_emb = self.c_emb(cate_seq_x[:,:,1]).view(bs, 80, -1)\n",
    "        seq_x = torch.cat((r_emb, c_emb, cont_seq_x), 2)\n",
    "        seq_emb = self.seq_emb(seq_x)\n",
    "        seq_emb, _ = self.lstm(seq_emb)\n",
    "        output = self.head(seq_emb).view(bs, -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# helper function\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    for step, (cate_seq_x, cont_seq_x, u_out, y) in enumerate(train_loader):\n",
    "        loss_mask = u_out == 0\n",
    "        cate_seq_x, cont_seq_x, y = cate_seq_x.to(device), cont_seq_x.to(device), y.to(device)\n",
    "        batch_size = cont_seq_x.size(0)\n",
    "        pred = model(cate_seq_x, cont_seq_x)\n",
    "        loss = 2. * criterion(pred[loss_mask], y[loss_mask]) + criterion(pred[loss_mask == 0], y[loss_mask == 0])\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        if CFG.apex:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader),\n",
    "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   loss=losses,\n",
    "                   grad_norm=grad_norm,\n",
    "                   lr=scheduler.get_lr()[0],\n",
    "                   ))\n",
    "        wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                   f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    for step, (cate_seq_x, cont_seq_x, u_out, y) in enumerate(valid_loader):\n",
    "        loss_mask = u_out == 0\n",
    "        cate_seq_x, cont_seq_x, y = cate_seq_x.to(device), cont_seq_x.to(device), y.to(device)\n",
    "        batch_size = cont_seq_x.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(cate_seq_x, cont_seq_x)\n",
    "        loss = 2. * criterion(pred[loss_mask], y[loss_mask]) + criterion(pred[loss_mask == 0], y[loss_mask == 0])\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(pred.view(-1).detach().cpu().numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader),\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   loss=losses,\n",
    "                   ))\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, (cate_seq_x, cont_seq_x) in tk0:\n",
    "        cate_seq_x, cont_seq_x = cate_seq_x.to(device), cont_seq_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(cate_seq_x, cont_seq_x)\n",
    "        preds.append(pred.view(-1).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "    \n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "    y_true = valid_folds['pressure'].values\n",
    "    non_expiratory_phase_val_idx = valid_folds[valid_folds['u_out'] == 0].index # The expiratory phase is not scored\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds)\n",
    "    valid_dataset = TrainDataset(valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG)\n",
    "    model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    \n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif CFG.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=CFG.num_cycles\n",
    "            )\n",
    "        elif CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # apex\n",
    "    # ====================================================\n",
    "    if CFG.apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(y_true[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - MAE Score (without expiratory phase): {score:.4f}')\n",
    "        wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                   f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                   f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                   f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'preds': preds},\n",
    "                        OUTPUT_DIR+f\"fold{fold}_best.pth\")\n",
    "            \n",
    "    preds = torch.load(OUTPUT_DIR+f\"fold{fold}_best.pth\", map_location=torch.device('cpu'))['preds']\n",
    "    valid_folds['preds'] = preds\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare: 1.train 2.test\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_result(result_df):\n",
    "        preds = result_df['preds'].values\n",
    "        labels = result_df['pressure'].values\n",
    "        non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n",
    "        score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n",
    "        LOGGER.info(f'Score (without expiratory phase): {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        # CV result\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        # save result\n",
    "        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n",
    "    \n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n",
    "        for fold in CFG.trn_fold:\n",
    "            model = CustomModel(CFG)\n",
    "            path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n",
    "            state = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "            predictions = inference_fn(test_loader, model, device)\n",
    "            test[f'fold{fold}'] = predictions\n",
    "            del state, predictions; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        # submission\n",
    "        test['pressure'] = test[[f'fold{fold}' for fold in range(CFG.n_fold)]].mean(1)\n",
    "        test[['id', 'pressure']+[f'fold{fold}' for fold in range(CFG.n_fold)]].to_csv(OUTPUT_DIR+'raw_submission.csv', index=False)\n",
    "        test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission.csv', index=False)\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 2m 9s (remain 1018m 6s) Loss: 43.5201(43.5201) Grad: 30.7354  LR: 0.007000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][100/471] Elapsed 2m 17s (remain 8m 25s) Loss: 5.8380(16.0565) Grad: 10.6756  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 2m 24s (remain 3m 14s) Loss: 4.1891(10.5941) Grad: 10.7566  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 2m 31s (remain 1m 25s) Loss: 4.0067(8.3990) Grad: 42.6261  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 2m 38s (remain 0m 27s) Loss: 3.8916(7.2296) Grad: 40.8703  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 2m 42s (remain 0m 0s) Loss: 3.9379(6.6871) Grad: 58.5869  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 18s (remain 152m 23s) Loss: 3.0252(3.0252) \n",
      "EVAL: [100/118] Elapsed 1m 23s (remain 0m 14s) Loss: 2.9052(2.9754) \n",
      "EVAL: [117/118] Elapsed 1m 24s (remain 0m 0s) Loss: 2.5755(2.9800) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.6871  avg_val_loss: 2.9800  time: 251s\n",
      "Epoch 1 - avg_train_loss: 6.6871  avg_val_loss: 2.9800  time: 251s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.2598\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.2598\n",
      "Epoch 1 - Save Best Score: 1.2598 Model\n",
      "Epoch 1 - Save Best Score: 1.2598 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 27s (remain 686m 58s) Loss: 3.3109(3.3109) Grad: 19.4190  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 34s (remain 5m 44s) Loss: 3.2151(3.3283) Grad: 34.8477  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 40s (remain 2m 15s) Loss: 3.5260(3.2658) Grad: 10.2927  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 47s (remain 1m 0s) Loss: 3.2469(3.1863) Grad: 28.1706  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 53s (remain 0m 19s) Loss: 2.6635(3.1641) Grad: 9.8127  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 57s (remain 0m 0s) Loss: 2.6549(3.1347) Grad: 13.7006  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 22s (remain 161m 39s) Loss: 2.7955(2.7955) \n",
      "EVAL: [100/118] Elapsed 1m 27s (remain 0m 14s) Loss: 2.7832(2.6042) \n",
      "EVAL: [117/118] Elapsed 1m 28s (remain 0m 0s) Loss: 2.3479(2.6027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.1347  avg_val_loss: 2.6027  time: 210s\n",
      "Epoch 2 - avg_train_loss: 3.1347  avg_val_loss: 2.6027  time: 210s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.0966\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.0966\n",
      "Epoch 2 - Save Best Score: 1.0966 Model\n",
      "Epoch 2 - Save Best Score: 1.0966 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 15s (remain 589m 3s) Loss: 2.5162(2.5162) Grad: 9.1405  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 21s (remain 4m 57s) Loss: 2.6658(2.8442) Grad: 30.4755  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 26s (remain 1m 56s) Loss: 3.0853(2.8576) Grad: 20.5815  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 32s (remain 0m 52s) Loss: 2.7643(2.8139) Grad: 30.7321  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 37s (remain 0m 17s) Loss: 2.6505(2.8127) Grad: 14.9742  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 41s (remain 0m 0s) Loss: 2.6337(2.8002) Grad: 24.3887  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 2s (remain 122m 31s) Loss: 2.7038(2.7038) \n",
      "EVAL: [100/118] Elapsed 1m 7s (remain 0m 11s) Loss: 2.7087(2.5181) \n",
      "EVAL: [117/118] Elapsed 1m 7s (remain 0m 0s) Loss: 2.3182(2.5123) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.8002  avg_val_loss: 2.5123  time: 173s\n",
      "Epoch 3 - avg_train_loss: 2.8002  avg_val_loss: 2.5123  time: 173s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0554\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0554\n",
      "Epoch 3 - Save Best Score: 1.0554 Model\n",
      "Epoch 3 - Save Best Score: 1.0554 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 13s (remain 576m 23s) Loss: 2.6593(2.6593) Grad: 30.5759  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 19s (remain 4m 50s) Loss: 2.6675(2.6571) Grad: 15.7047  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 24s (remain 1m 53s) Loss: 2.4535(2.6253) Grad: 23.1936  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 30s (remain 0m 50s) Loss: 2.3036(2.6484) Grad: 16.9687  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 35s (remain 0m 16s) Loss: 2.8166(2.6424) Grad: 31.4900  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 39s (remain 0m 0s) Loss: 2.6088(2.6393) Grad: 10.3658  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 3s (remain 122m 54s) Loss: 3.0185(3.0185) \n",
      "EVAL: [100/118] Elapsed 1m 7s (remain 0m 11s) Loss: 2.7975(2.7083) \n",
      "EVAL: [117/118] Elapsed 1m 7s (remain 0m 0s) Loss: 2.5039(2.6994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.6393  avg_val_loss: 2.6994  time: 172s\n",
      "Epoch 4 - avg_train_loss: 2.6393  avg_val_loss: 2.6994  time: 172s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.1583\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.1583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 13s (remain 577m 25s) Loss: 2.6923(2.6923) Grad: 23.0344  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 19s (remain 4m 50s) Loss: 2.4381(2.6074) Grad: 37.4635  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 24s (remain 1m 53s) Loss: 2.6415(2.6019) Grad: 13.5991  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 29s (remain 0m 50s) Loss: 2.4200(2.5971) Grad: 8.4229  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 35s (remain 0m 16s) Loss: 2.6189(2.5752) Grad: 10.8092  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 39s (remain 0m 0s) Loss: 2.3494(2.5607) Grad: 8.8655  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 2s (remain 122m 50s) Loss: 2.7960(2.7960) \n",
      "EVAL: [100/118] Elapsed 1m 7s (remain 0m 11s) Loss: 2.4829(2.4429) \n",
      "EVAL: [117/118] Elapsed 1m 7s (remain 0m 0s) Loss: 2.1735(2.4371) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5607  avg_val_loss: 2.4371  time: 171s\n",
      "Epoch 5 - avg_train_loss: 2.5607  avg_val_loss: 2.4371  time: 171s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 1.0241\n",
      "Epoch 5 - MAE Score (without expiratory phase): 1.0241\n",
      "Epoch 5 - Save Best Score: 1.0241 Model\n",
      "Epoch 5 - Save Best Score: 1.0241 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/471] Elapsed 1m 13s (remain 575m 11s) Loss: 2.3915(2.3915) Grad: 34.2486  LR: 0.006768  \n",
      "Epoch: [6][100/471] Elapsed 1m 19s (remain 4m 49s) Loss: 2.3302(2.4664) Grad: 10.9589  LR: 0.006768  \n",
      "Epoch: [6][200/471] Elapsed 1m 24s (remain 1m 53s) Loss: 2.2661(2.4518) Grad: 15.9376  LR: 0.006768  \n",
      "Epoch: [6][300/471] Elapsed 1m 30s (remain 0m 50s) Loss: 2.4166(2.4376) Grad: 28.9850  LR: 0.006768  \n",
      "Epoch: [6][400/471] Elapsed 1m 35s (remain 0m 16s) Loss: 2.5489(2.4533) Grad: 19.9196  LR: 0.006768  \n",
      "Epoch: [6][470/471] Elapsed 1m 38s (remain 0m 0s) Loss: 2.5498(2.4389) Grad: 32.5794  LR: 0.006768  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 120m 25s) Loss: 2.2384(2.2384) \n",
      "EVAL: [100/118] Elapsed 1m 5s (remain 0m 11s) Loss: 2.0980(2.1345) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 1.9867(2.1347) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 2.4389  avg_val_loss: 2.1347  time: 169s\n",
      "Epoch 6 - avg_train_loss: 2.4389  avg_val_loss: 2.1347  time: 169s\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.8899\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.8899\n",
      "Epoch 6 - Save Best Score: 0.8899 Model\n",
      "Epoch 6 - Save Best Score: 0.8899 Model\n",
      "========== fold: 0 result ==========\n",
      "========== fold: 0 result ==========\n",
      "Score (without expiratory phase): 0.8899\n",
      "Score (without expiratory phase): 0.8899\n",
      "========== fold: 1 training ==========\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 10s (remain 552m 36s) Loss: 43.3375(43.3375) Grad: 32.6604  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 15s (remain 4m 38s) Loss: 6.7774(15.7762) Grad: 35.4159  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 21s (remain 1m 49s) Loss: 4.7029(10.7021) Grad: 34.1160  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 26s (remain 0m 48s) Loss: 3.5493(8.5899) Grad: 10.6257  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 31s (remain 0m 15s) Loss: 3.7331(7.3853) Grad: 44.3133  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 34s (remain 0m 0s) Loss: 3.7330(6.8159) Grad: 27.2789  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 0s (remain 118m 5s) Loss: 3.3602(3.3602) \n",
      "EVAL: [100/118] Elapsed 1m 4s (remain 0m 10s) Loss: 3.0041(3.2394) \n",
      "EVAL: [117/118] Elapsed 1m 5s (remain 0m 0s) Loss: 2.9083(3.2312) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.8159  avg_val_loss: 3.2312  time: 164s\n",
      "Epoch 1 - avg_train_loss: 6.8159  avg_val_loss: 3.2312  time: 164s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.3737\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.3737\n",
      "Epoch 1 - Save Best Score: 1.3737 Model\n",
      "Epoch 1 - Save Best Score: 1.3737 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 10s (remain 551m 15s) Loss: 3.7693(3.7693) Grad: 15.1866  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 15s (remain 4m 37s) Loss: 3.2502(3.4935) Grad: 11.9221  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 21s (remain 1m 48s) Loss: 3.1517(3.3509) Grad: 16.6973  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 25s (remain 0m 48s) Loss: 3.0046(3.3695) Grad: 32.5639  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 31s (remain 0m 15s) Loss: 2.9671(3.2963) Grad: 15.6600  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 34s (remain 0m 0s) Loss: 2.9592(3.2518) Grad: 17.2189  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 13s (remain 142m 36s) Loss: 2.6840(2.6840) \n",
      "EVAL: [100/118] Elapsed 1m 18s (remain 0m 13s) Loss: 2.4052(2.6287) \n",
      "EVAL: [117/118] Elapsed 1m 19s (remain 0m 0s) Loss: 2.3682(2.6232) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.2518  avg_val_loss: 2.6232  time: 178s\n",
      "Epoch 2 - avg_train_loss: 3.2518  avg_val_loss: 2.6232  time: 178s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1047\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1047\n",
      "Epoch 2 - Save Best Score: 1.1047 Model\n",
      "Epoch 2 - Save Best Score: 1.1047 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 29s (remain 698m 29s) Loss: 2.7907(2.7907) Grad: 9.1141  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 37s (remain 5m 57s) Loss: 2.5050(2.9226) Grad: 12.2533  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 45s (remain 2m 21s) Loss: 2.9254(2.9339) Grad: 23.8556  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 52s (remain 1m 3s) Loss: 2.6046(2.9030) Grad: 28.8644  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 2m 0s (remain 0m 21s) Loss: 3.1205(2.8862) Grad: 44.4569  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 2m 4s (remain 0m 0s) Loss: 3.3590(2.9048) Grad: 14.6612  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 22s (remain 160m 12s) Loss: 2.5874(2.5874) \n",
      "EVAL: [100/118] Elapsed 1m 28s (remain 0m 14s) Loss: 2.7731(2.8897) \n",
      "EVAL: [117/118] Elapsed 1m 28s (remain 0m 0s) Loss: 2.3776(2.8800) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.9048  avg_val_loss: 2.8800  time: 219s\n",
      "Epoch 3 - avg_train_loss: 2.9048  avg_val_loss: 2.8800  time: 219s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.1888\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.1888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 33s (remain 730m 9s) Loss: 3.0786(3.0786) Grad: 12.8761  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 41s (remain 6m 12s) Loss: 2.4552(2.8254) Grad: 10.0772  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 50s (remain 2m 28s) Loss: 2.5871(2.8086) Grad: 18.3257  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 59s (remain 1m 7s) Loss: 2.7302(2.7816) Grad: 18.3936  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 2m 7s (remain 0m 22s) Loss: 2.7110(2.7633) Grad: 21.6084  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 2m 11s (remain 0m 0s) Loss: 3.0224(2.7565) Grad: 43.8968  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 12s (remain 140m 51s) Loss: 2.2047(2.2047) \n",
      "EVAL: [100/118] Elapsed 1m 17s (remain 0m 13s) Loss: 2.2081(2.3398) \n",
      "EVAL: [117/118] Elapsed 1m 18s (remain 0m 0s) Loss: 2.0868(2.3385) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.7565  avg_val_loss: 2.3385  time: 215s\n",
      "Epoch 4 - avg_train_loss: 2.7565  avg_val_loss: 2.3385  time: 215s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 0.9753\n",
      "Epoch 4 - MAE Score (without expiratory phase): 0.9753\n",
      "Epoch 4 - Save Best Score: 0.9753 Model\n",
      "Epoch 4 - Save Best Score: 0.9753 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 25s (remain 669m 10s) Loss: 2.4004(2.4004) Grad: 13.5608  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 31s (remain 5m 35s) Loss: 2.5232(2.7602) Grad: 23.9114  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 37s (remain 2m 11s) Loss: 2.5626(2.6875) Grad: 38.3614  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 43s (remain 0m 58s) Loss: 2.6410(2.6324) Grad: 8.5186  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 49s (remain 0m 19s) Loss: 2.3898(2.6024) Grad: 22.2102  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 52s (remain 0m 0s) Loss: 2.6071(2.5861) Grad: 16.8815  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 8s (remain 132m 53s) Loss: 2.1424(2.1424) \n",
      "EVAL: [100/118] Elapsed 1m 14s (remain 0m 12s) Loss: 2.2361(2.3148) \n",
      "EVAL: [117/118] Elapsed 1m 14s (remain 0m 0s) Loss: 2.0380(2.3127) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5861  avg_val_loss: 2.3127  time: 192s\n",
      "Epoch 5 - avg_train_loss: 2.5861  avg_val_loss: 2.3127  time: 192s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9683\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9683\n",
      "Epoch 5 - Save Best Score: 0.9683 Model\n",
      "Epoch 5 - Save Best Score: 0.9683 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/471] Elapsed 1m 25s (remain 671m 0s) Loss: 2.2685(2.2685) Grad: 8.0980  LR: 0.006768  \n",
      "Epoch: [6][100/471] Elapsed 1m 32s (remain 5m 40s) Loss: 2.4086(2.5032) Grad: 8.7661  LR: 0.006768  \n",
      "Epoch: [6][200/471] Elapsed 1m 40s (remain 2m 15s) Loss: 2.5129(2.5371) Grad: 31.6646  LR: 0.006768  \n",
      "Epoch: [6][300/471] Elapsed 1m 49s (remain 1m 1s) Loss: 2.3184(2.5418) Grad: 9.2018  LR: 0.006768  \n",
      "Epoch: [6][400/471] Elapsed 1m 56s (remain 0m 20s) Loss: 2.5929(2.5186) Grad: 18.3270  LR: 0.006768  \n",
      "Epoch: [6][470/471] Elapsed 2m 1s (remain 0m 0s) Loss: 1.9867(2.4973) Grad: 13.5933  LR: 0.006768  \n",
      "EVAL: [0/118] Elapsed 1m 12s (remain 141m 29s) Loss: 1.9939(1.9939) \n",
      "EVAL: [100/118] Elapsed 1m 17s (remain 0m 13s) Loss: 2.0904(2.1643) \n",
      "EVAL: [117/118] Elapsed 1m 17s (remain 0m 0s) Loss: 1.8053(2.1626) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 2.4973  avg_val_loss: 2.1626  time: 204s\n",
      "Epoch 6 - avg_train_loss: 2.4973  avg_val_loss: 2.1626  time: 204s\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.8991\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.8991\n",
      "Epoch 6 - Save Best Score: 0.8991 Model\n",
      "Epoch 6 - Save Best Score: 0.8991 Model\n",
      "========== fold: 1 result ==========\n",
      "========== fold: 1 result ==========\n",
      "Score (without expiratory phase): 0.8991\n",
      "Score (without expiratory phase): 0.8991\n",
      "========== fold: 2 training ==========\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 13s (remain 577m 6s) Loss: 41.5169(41.5169) Grad: 31.6657  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 19s (remain 4m 49s) Loss: 6.4613(16.2058) Grad: 17.0646  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 24s (remain 1m 53s) Loss: 3.9896(10.7442) Grad: 6.5867  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 30s (remain 0m 50s) Loss: 3.2338(8.5419) Grad: 8.6603  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 35s (remain 0m 16s) Loss: 3.7988(7.3404) Grad: 45.0048  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 39s (remain 0m 0s) Loss: 3.5106(6.7862) Grad: 12.9447  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 3s (remain 123m 11s) Loss: 3.7217(3.7217) \n",
      "EVAL: [100/118] Elapsed 1m 7s (remain 0m 11s) Loss: 3.4137(3.3803) \n",
      "EVAL: [117/118] Elapsed 1m 8s (remain 0m 0s) Loss: 2.9371(3.3926) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.7862  avg_val_loss: 3.3926  time: 171s\n",
      "Epoch 1 - avg_train_loss: 6.7862  avg_val_loss: 3.3926  time: 171s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4460\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4460\n",
      "Epoch 1 - Save Best Score: 1.4460 Model\n",
      "Epoch 1 - Save Best Score: 1.4460 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 16s (remain 601m 42s) Loss: 3.5906(3.5906) Grad: 27.7806  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 24s (remain 5m 10s) Loss: 3.0873(3.2776) Grad: 15.4336  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 31s (remain 2m 2s) Loss: 3.0815(3.3395) Grad: 20.0429  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 38s (remain 0m 55s) Loss: 2.7859(3.2387) Grad: 12.2306  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 45s (remain 0m 18s) Loss: 2.7601(3.1938) Grad: 14.7065  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 50s (remain 0m 0s) Loss: 3.0620(3.1646) Grad: 17.5896  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 13s (remain 143m 27s) Loss: 3.0321(3.0321) \n",
      "EVAL: [100/118] Elapsed 1m 18s (remain 0m 13s) Loss: 2.6708(2.7413) \n",
      "EVAL: [117/118] Elapsed 1m 19s (remain 0m 0s) Loss: 2.4293(2.7530) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.1646  avg_val_loss: 2.7530  time: 194s\n",
      "Epoch 2 - avg_train_loss: 3.1646  avg_val_loss: 2.7530  time: 194s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1349\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1349\n",
      "Epoch 2 - Save Best Score: 1.1349 Model\n",
      "Epoch 2 - Save Best Score: 1.1349 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 40s (remain 788m 11s) Loss: 3.0527(3.0527) Grad: 11.0611  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 49s (remain 6m 42s) Loss: 2.9265(2.8914) Grad: 14.1906  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 57s (remain 2m 38s) Loss: 2.8296(2.8452) Grad: 35.0611  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 2m 6s (remain 1m 11s) Loss: 2.9877(2.8402) Grad: 30.7687  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 2m 17s (remain 0m 23s) Loss: 2.7890(2.8169) Grad: 24.9383  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 2m 23s (remain 0m 0s) Loss: 2.6669(2.8046) Grad: 28.9782  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 21s (remain 159m 8s) Loss: 2.5177(2.5177) \n",
      "EVAL: [100/118] Elapsed 1m 27s (remain 0m 14s) Loss: 2.2802(2.4091) \n",
      "EVAL: [117/118] Elapsed 1m 27s (remain 0m 0s) Loss: 2.1466(2.4212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.8046  avg_val_loss: 2.4212  time: 237s\n",
      "Epoch 3 - avg_train_loss: 2.8046  avg_val_loss: 2.4212  time: 237s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0113\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0113\n",
      "Epoch 3 - Save Best Score: 1.0113 Model\n",
      "Epoch 3 - Save Best Score: 1.0113 Model\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
