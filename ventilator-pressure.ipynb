{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook  \n",
    "- PyTorch RNN starter code with W&B  \n",
    "- Pytorch W&B Usage Examples from https://docs.wandb.ai/guides/integrations/pytorch  \n",
    "\n",
    "If this notebook is helpful, feel free to upvote :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/google/deluca-lung/main/assets/2020-10-02%20Ventilator%20diagram.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    competition='ventilator'\n",
    "    _wandb_kernel='nakama'\n",
    "    apex=False\n",
    "    print_freq=100\n",
    "    num_workers=10\n",
    "    model_name='rnn'\n",
    "    scheduler='CosineAnnealingLR' # ['linear', 'cosine', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    batch_scheduler=False\n",
    "    #num_warmup_steps=100 # ['linear', 'cosine']\n",
    "    #num_cycles=0.5 # 'cosine'\n",
    "    #factor=0.2 # ReduceLROnPlateau\n",
    "    #patience=4 # ReduceLROnPlateau\n",
    "    #eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=50 # CosineAnnealingLR\n",
    "    #T_0=50 # CosineAnnealingWarmRestarts\n",
    "    epochs=6\n",
    "    max_grad_norm=1000\n",
    "    gradient_accumulation_steps=1\n",
    "    hidden_size=64\n",
    "    lr=7e-3\n",
    "    min_lr=2e-6\n",
    "    weight_decay=1e-6\n",
    "    batch_size=128\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    cate_seq_cols=['R', 'C']\n",
    "    cont_seq_cols=['time_step', 'u_in', 'u_out'] + ['breath_time', 'u_in_time']\n",
    "    train=True\n",
    "    inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if CFG.apex:\n",
    "    from apex import amp\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: remgalleu (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \n",
      "Get your W&B access token from here: https://wandb.ai/authorize\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=secret_value_0)\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "    \n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "run = wandb.init(project=\"Ventilator-Pressure-Public\", \n",
    "                 name=CFG.model_name,\n",
    "                 config=class2dict(CFG),\n",
    "                 group=CFG.model_name,\n",
    "                 job_type=\"train\",\n",
    "                 anonymous=anony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_trues, y_preds):\n",
    "    score = mean_absolute_error(y_trues, y_preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080043</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>2.964399</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>3.157395</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.101542</td>\n",
       "      <td>3.170056</td>\n",
       "      <td>0</td>\n",
       "      <td>11.742872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>3.271690</td>\n",
       "      <td>0</td>\n",
       "      <td>12.234987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  R  C  time_step      u_in  u_out   pressure\n",
       "0   1          1  1  2   0.000000  0.080043      0   5.837492\n",
       "1   2          1  1  2   0.033652  2.964399      0   5.907794\n",
       "2   3          1  1  2   0.067514  3.157395      0   7.876254\n",
       "3   4          1  1  2   0.101542  3.170056      0  11.742872\n",
       "4   5          1  1  2   0.135756  3.271690      0  12.234987"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031904</td>\n",
       "      <td>2.141835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063827</td>\n",
       "      <td>2.750578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.095751</td>\n",
       "      <td>3.101470</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.127644</td>\n",
       "      <td>3.307654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  R  C  time_step      u_in  u_out\n",
       "0   1          0  0  1   0.000000  0.000000      0\n",
       "1   2          0  0  1   0.031904  2.141835      0\n",
       "2   3          0  0  1   0.063827  2.750578      0\n",
       "3   4          0  0  1   0.095751  3.101470      0\n",
       "4   5          0  0  1   0.127644  3.307654      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  pressure\n",
       "0   1         0\n",
       "1   2         0\n",
       "2   3         0\n",
       "3   4         0\n",
       "4   5         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "for c in ['u_in']:\n",
    "    train[c] = np.log1p(train[c])\n",
    "    test[c] = np.log1p(test[c])\n",
    "    \n",
    "r_map = {5: 0, 20: 1, 50: 2}\n",
    "c_map = {10: 0, 20: 1, 50: 2}\n",
    "train['R'] = train['R'].map(r_map)\n",
    "test['R'] = test['R'].map(r_map)\n",
    "train['C'] = train['C'].map(c_map)\n",
    "test['C'] = test['C'].map(c_map)\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# FE\n",
    "# ====================================================\n",
    "def add_feature(df):\n",
    "    # breath_time\n",
    "    df['breath_time'] = df['time_step'] - df['time_step'].shift(1)\n",
    "    df.loc[df['time_step'] == 0, 'breath_time'] = 0\n",
    "    # u_in_time\n",
    "    df['u_in_time'] = df['u_in'] - df['u_in'].shift(1)\n",
    "    df.loc[df['time_step'] == 0, 'u_in_time'] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "train = add_feature(train)\n",
    "test = add_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n",
      "0    1207200\n",
      "1    1207200\n",
      "2    1207200\n",
      "3    1207200\n",
      "4    1207200\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=5)\n",
    "groups = train['breath_id'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['pressure'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "print(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.hidden_size = cfg.hidden_size\n",
    "        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.seq_emb = nn.Sequential(\n",
    "            nn.Linear(4 + len(cfg.cont_seq_cols), self.hidden_size),\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, \n",
    "                            dropout=0.2, batch_first=True, bidirectional=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 2, self.hidden_size * 2),\n",
    "            nn.LayerNorm(self.hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.),\n",
    "            nn.Linear(self.hidden_size * 2, 1),\n",
    "        )\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                print(f'init {m}')\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        nn.init.normal_(param.data)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                print(f\"init {m}\")\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        init.normal_(param.data)\n",
    "\n",
    "    def forward(self, cate_seq_x, cont_seq_x):\n",
    "        bs = cont_seq_x.size(0)\n",
    "        r_emb = self.r_emb(cate_seq_x[:,:,0]).view(bs, 80, -1)\n",
    "        c_emb = self.c_emb(cate_seq_x[:,:,1]).view(bs, 80, -1)\n",
    "        seq_x = torch.cat((r_emb, c_emb, cont_seq_x), 2)\n",
    "        seq_emb = self.seq_emb(seq_x)\n",
    "        seq_emb, _ = self.lstm(seq_emb)\n",
    "        output = self.head(seq_emb).view(bs, -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# helper function\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    for step, (cate_seq_x, cont_seq_x, u_out, y) in enumerate(train_loader):\n",
    "        loss_mask = u_out == 0\n",
    "        cate_seq_x, cont_seq_x, y = cate_seq_x.to(device), cont_seq_x.to(device), y.to(device)\n",
    "        batch_size = cont_seq_x.size(0)\n",
    "        pred = model(cate_seq_x, cont_seq_x)\n",
    "        loss = 2. * criterion(pred[loss_mask], y[loss_mask]) + criterion(pred[loss_mask == 0], y[loss_mask == 0])\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        if CFG.apex:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader),\n",
    "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   loss=losses,\n",
    "                   grad_norm=grad_norm,\n",
    "                   lr=scheduler.get_lr()[0],\n",
    "                   ))\n",
    "        wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                   f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    for step, (cate_seq_x, cont_seq_x, u_out, y) in enumerate(valid_loader):\n",
    "        loss_mask = u_out == 0\n",
    "        cate_seq_x, cont_seq_x, y = cate_seq_x.to(device), cont_seq_x.to(device), y.to(device)\n",
    "        batch_size = cont_seq_x.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(cate_seq_x, cont_seq_x)\n",
    "        loss = 2. * criterion(pred[loss_mask], y[loss_mask]) + criterion(pred[loss_mask == 0], y[loss_mask == 0])\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(pred.view(-1).detach().cpu().numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader),\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   loss=losses,\n",
    "                   ))\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, (cate_seq_x, cont_seq_x) in tk0:\n",
    "        cate_seq_x, cont_seq_x = cate_seq_x.to(device), cont_seq_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(cate_seq_x, cont_seq_x)\n",
    "        preds.append(pred.view(-1).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "    \n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "    y_true = valid_folds['pressure'].values\n",
    "    non_expiratory_phase_val_idx = valid_folds[valid_folds['u_out'] == 0].index # The expiratory phase is not scored\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds)\n",
    "    valid_dataset = TrainDataset(valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG)\n",
    "    model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    \n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif CFG.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=CFG.num_cycles\n",
    "            )\n",
    "        elif CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # apex\n",
    "    # ====================================================\n",
    "    if CFG.apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(y_true[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - MAE Score (without expiratory phase): {score:.4f}')\n",
    "        wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                   f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                   f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                   f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'preds': preds},\n",
    "                        OUTPUT_DIR+f\"fold{fold}_best.pth\")\n",
    "            \n",
    "    preds = torch.load(OUTPUT_DIR+f\"fold{fold}_best.pth\", map_location=torch.device('cpu'))['preds']\n",
    "    valid_folds['preds'] = preds\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare: 1.train 2.test\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_result(result_df):\n",
    "        preds = result_df['preds'].values\n",
    "        labels = result_df['pressure'].values\n",
    "        non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n",
    "        score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n",
    "        LOGGER.info(f'Score (without expiratory phase): {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        # CV result\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        # save result\n",
    "        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n",
    "    \n",
    "    if CFG.inference:\n",
    "        test_dataset = TestDataset(test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n",
    "        for fold in CFG.trn_fold:\n",
    "            model = CustomModel(CFG)\n",
    "            path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n",
    "            state = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "            predictions = inference_fn(test_loader, model, device)\n",
    "            test[f'fold{fold}'] = predictions\n",
    "            del state, predictions; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        # submission\n",
    "        test['pressure'] = test[[f'fold{fold}' for fold in range(CFG.n_fold)]].mean(1)\n",
    "        test[['id', 'pressure']+[f'fold{fold}' for fold in range(CFG.n_fold)]].to_csv(OUTPUT_DIR+'raw_submission.csv', index=False)\n",
    "        test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission.csv', index=False)\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 32s (remain 723m 36s) Loss: 43.5201(43.5201) Grad: 30.7354  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 39s (remain 6m 2s) Loss: 5.7521(16.0507) Grad: 16.3169  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 44s (remain 2m 20s) Loss: 4.2114(10.5726) Grad: 23.1364  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 50s (remain 1m 2s) Loss: 3.9576(8.3976) Grad: 28.9891  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 56s (remain 0m 20s) Loss: 3.5322(7.2115) Grad: 20.0299  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 59s (remain 0m 0s) Loss: 3.7771(6.6540) Grad: 54.5168  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 55s) Loss: 3.4728(3.4728) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 3.3713(3.2843) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 2.8458(3.2790) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.6540  avg_val_loss: 3.2790  time: 195s\n",
      "Epoch 1 - avg_train_loss: 6.6540  avg_val_loss: 3.2790  time: 195s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4120\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4120\n",
      "Epoch 1 - Save Best Score: 1.4120 Model\n",
      "Epoch 1 - Save Best Score: 1.4120 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 15s (remain 594m 57s) Loss: 3.5236(3.5236) Grad: 40.1634  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 21s (remain 5m 0s) Loss: 3.0845(3.2422) Grad: 22.8498  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 27s (remain 1m 57s) Loss: 2.7807(3.2376) Grad: 20.6130  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 33s (remain 0m 52s) Loss: 3.1369(3.1979) Grad: 14.9282  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 39s (remain 0m 17s) Loss: 2.9431(3.1645) Grad: 32.2216  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 43s (remain 0m 0s) Loss: 2.6617(3.1337) Grad: 19.4457  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 6s (remain 129m 54s) Loss: 2.6762(2.6762) \n",
      "EVAL: [100/118] Elapsed 1m 11s (remain 0m 11s) Loss: 2.6562(2.5522) \n",
      "EVAL: [117/118] Elapsed 1m 11s (remain 0m 0s) Loss: 2.3295(2.5541) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.1337  avg_val_loss: 2.5541  time: 180s\n",
      "Epoch 2 - avg_train_loss: 3.1337  avg_val_loss: 2.5541  time: 180s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.0735\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.0735\n",
      "Epoch 2 - Save Best Score: 1.0735 Model\n",
      "Epoch 2 - Save Best Score: 1.0735 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 15s (remain 592m 57s) Loss: 2.8309(2.8309) Grad: 15.1804  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 22s (remain 5m 1s) Loss: 2.6720(2.8931) Grad: 9.9345  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 27s (remain 1m 57s) Loss: 2.8867(2.8634) Grad: 24.3714  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 33s (remain 0m 53s) Loss: 2.6967(2.8624) Grad: 9.2108  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 39s (remain 0m 17s) Loss: 2.9481(2.8350) Grad: 44.8964  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 43s (remain 0m 0s) Loss: 2.6882(2.8370) Grad: 19.3364  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 3s) Loss: 2.6107(2.6107) \n",
      "EVAL: [100/118] Elapsed 1m 9s (remain 0m 11s) Loss: 2.5310(2.4784) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 2.3087(2.4775) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.8370  avg_val_loss: 2.4775  time: 179s\n",
      "Epoch 3 - avg_train_loss: 2.8370  avg_val_loss: 2.4775  time: 179s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0442\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0442\n",
      "Epoch 3 - Save Best Score: 1.0442 Model\n",
      "Epoch 3 - Save Best Score: 1.0442 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 15s (remain 593m 34s) Loss: 2.6449(2.6449) Grad: 15.7926  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 21s (remain 4m 59s) Loss: 2.3982(2.7374) Grad: 10.9020  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 27s (remain 1m 57s) Loss: 2.6073(2.7130) Grad: 29.8263  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 33s (remain 0m 52s) Loss: 2.5682(2.6905) Grad: 14.2282  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 39s (remain 0m 17s) Loss: 2.4580(2.6635) Grad: 11.8174  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 42s (remain 0m 0s) Loss: 3.0277(2.6701) Grad: 54.2574  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 125m 57s) Loss: 2.5140(2.5140) \n",
      "EVAL: [100/118] Elapsed 1m 9s (remain 0m 11s) Loss: 2.4958(2.3183) \n",
      "EVAL: [117/118] Elapsed 1m 9s (remain 0m 0s) Loss: 2.0368(2.3158) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.6701  avg_val_loss: 2.3158  time: 177s\n",
      "Epoch 4 - avg_train_loss: 2.6701  avg_val_loss: 2.3158  time: 177s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 0.9674\n",
      "Epoch 4 - MAE Score (without expiratory phase): 0.9674\n",
      "Epoch 4 - Save Best Score: 0.9674 Model\n",
      "Epoch 4 - Save Best Score: 0.9674 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 15s (remain 589m 39s) Loss: 2.4731(2.4731) Grad: 20.3619  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 21s (remain 4m 57s) Loss: 2.6483(2.6349) Grad: 18.5833  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 27s (remain 1m 57s) Loss: 2.7194(2.5973) Grad: 38.1790  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 33s (remain 0m 52s) Loss: 2.3389(2.6135) Grad: 26.2337  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 39s (remain 0m 17s) Loss: 2.5929(2.6013) Grad: 19.0850  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 42s (remain 0m 0s) Loss: 2.3641(2.5795) Grad: 13.5729  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 125m 48s) Loss: 2.4832(2.4832) \n",
      "EVAL: [100/118] Elapsed 1m 9s (remain 0m 11s) Loss: 2.3070(2.2278) \n",
      "EVAL: [117/118] Elapsed 1m 9s (remain 0m 0s) Loss: 1.9338(2.2296) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5795  avg_val_loss: 2.2296  time: 177s\n",
      "Epoch 5 - avg_train_loss: 2.5795  avg_val_loss: 2.2296  time: 177s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9323\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9323\n",
      "Epoch 5 - Save Best Score: 0.9323 Model\n",
      "Epoch 5 - Save Best Score: 0.9323 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/471] Elapsed 1m 14s (remain 585m 43s) Loss: 2.5337(2.5337) Grad: 8.1071  LR: 0.006768  \n",
      "Epoch: [6][100/471] Elapsed 1m 20s (remain 4m 55s) Loss: 2.2862(2.5187) Grad: 10.8556  LR: 0.006768  \n",
      "Epoch: [6][200/471] Elapsed 1m 25s (remain 1m 55s) Loss: 2.7150(2.5426) Grad: 33.5310  LR: 0.006768  \n",
      "Epoch: [6][300/471] Elapsed 1m 31s (remain 0m 51s) Loss: 2.3895(2.5156) Grad: 9.0836  LR: 0.006768  \n",
      "Epoch: [6][400/471] Elapsed 1m 36s (remain 0m 16s) Loss: 2.2490(2.4983) Grad: 20.3949  LR: 0.006768  \n",
      "Epoch: [6][470/471] Elapsed 1m 39s (remain 0m 0s) Loss: 2.2677(2.4779) Grad: 7.9676  LR: 0.006768  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 120m 0s) Loss: 2.4315(2.4315) \n",
      "EVAL: [100/118] Elapsed 1m 5s (remain 0m 11s) Loss: 2.4150(2.3075) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 2.2099(2.3048) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 2.4779  avg_val_loss: 2.3048  time: 171s\n",
      "Epoch 6 - avg_train_loss: 2.4779  avg_val_loss: 2.3048  time: 171s\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.9654\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.9654\n",
      "========== fold: 0 result ==========\n",
      "========== fold: 0 result ==========\n",
      "Score (without expiratory phase): 0.9323\n",
      "Score (without expiratory phase): 0.9323\n",
      "========== fold: 1 training ==========\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 11s (remain 560m 42s) Loss: 45.5942(45.5942) Grad: 32.4443  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 17s (remain 4m 42s) Loss: 7.3883(15.6969) Grad: 56.1525  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 22s (remain 1m 50s) Loss: 4.3390(10.4452) Grad: 15.9344  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 27s (remain 0m 49s) Loss: 3.9418(8.3843) Grad: 12.6984  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 33s (remain 0m 16s) Loss: 3.4559(7.2379) Grad: 6.4458  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 37s (remain 0m 0s) Loss: 3.5188(6.6889) Grad: 25.9323  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 120m 26s) Loss: 4.1860(4.1860) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 3.6979(3.8880) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 3.3469(3.8850) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.6889  avg_val_loss: 3.8850  time: 168s\n",
      "Epoch 1 - avg_train_loss: 6.6889  avg_val_loss: 3.8850  time: 168s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.6888\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.6888\n",
      "Epoch 1 - Save Best Score: 1.6888 Model\n",
      "Epoch 1 - Save Best Score: 1.6888 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 13s (remain 576m 40s) Loss: 4.0365(4.0365) Grad: 56.5724  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 19s (remain 4m 49s) Loss: 3.1584(3.4882) Grad: 16.4691  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 24s (remain 1m 53s) Loss: 2.9253(3.3489) Grad: 15.3204  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 29s (remain 0m 50s) Loss: 3.0155(3.2908) Grad: 23.4289  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 34s (remain 0m 16s) Loss: 2.6844(3.2282) Grad: 16.7420  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 38s (remain 0m 0s) Loss: 2.7107(3.1899) Grad: 22.2608  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 119m 16s) Loss: 2.5192(2.5192) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 2.4828(2.5906) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 2.2760(2.5885) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.1899  avg_val_loss: 2.5885  time: 169s\n",
      "Epoch 2 - avg_train_loss: 3.1899  avg_val_loss: 2.5885  time: 169s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.0904\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.0904\n",
      "Epoch 2 - Save Best Score: 1.0904 Model\n",
      "Epoch 2 - Save Best Score: 1.0904 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 12s (remain 565m 55s) Loss: 2.8174(2.8174) Grad: 8.3949  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 17s (remain 4m 44s) Loss: 2.8624(3.0156) Grad: 25.9067  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 22s (remain 1m 51s) Loss: 2.7193(2.9574) Grad: 11.5561  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 28s (remain 0m 49s) Loss: 2.9242(2.9092) Grad: 22.2743  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 33s (remain 0m 16s) Loss: 2.9279(2.8852) Grad: 21.4675  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 36s (remain 0m 0s) Loss: 2.9009(2.8703) Grad: 17.8781  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 120m 37s) Loss: 2.6550(2.6550) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 2.4023(2.5760) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 2.1246(2.5701) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.8703  avg_val_loss: 2.5701  time: 168s\n",
      "Epoch 3 - avg_train_loss: 2.8703  avg_val_loss: 2.5701  time: 168s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0895\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0895\n",
      "Epoch 3 - Save Best Score: 1.0895 Model\n",
      "Epoch 3 - Save Best Score: 1.0895 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 12s (remain 565m 4s) Loss: 2.7332(2.7332) Grad: 28.7837  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 17s (remain 4m 44s) Loss: 2.8689(2.7674) Grad: 16.0320  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 23s (remain 1m 51s) Loss: 2.8527(2.7132) Grad: 12.9903  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 28s (remain 0m 50s) Loss: 2.6307(2.7029) Grad: 22.4252  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 34s (remain 0m 16s) Loss: 2.6373(2.6924) Grad: 9.1786  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 37s (remain 0m 0s) Loss: 3.0183(2.6827) Grad: 32.4578  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 118m 58s) Loss: 2.4479(2.4479) \n",
      "EVAL: [100/118] Elapsed 1m 5s (remain 0m 11s) Loss: 2.3844(2.5273) \n",
      "EVAL: [117/118] Elapsed 1m 5s (remain 0m 0s) Loss: 2.2725(2.5244) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.6827  avg_val_loss: 2.5244  time: 167s\n",
      "Epoch 4 - avg_train_loss: 2.6827  avg_val_loss: 2.5244  time: 167s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0588\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0588\n",
      "Epoch 4 - Save Best Score: 1.0588 Model\n",
      "Epoch 4 - Save Best Score: 1.0588 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 12s (remain 565m 33s) Loss: 2.6040(2.6040) Grad: 10.1955  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 17s (remain 4m 44s) Loss: 2.2165(2.6887) Grad: 6.2312  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 22s (remain 1m 51s) Loss: 2.6091(2.6043) Grad: 15.6832  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 28s (remain 0m 49s) Loss: 2.5174(2.5851) Grad: 8.3494  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 33s (remain 0m 16s) Loss: 2.5768(2.5947) Grad: 10.9835  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 37s (remain 0m 0s) Loss: 2.5211(2.5969) Grad: 21.4604  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 2s (remain 121m 9s) Loss: 2.2767(2.2767) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 2.1710(2.3075) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 1.9597(2.2998) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5969  avg_val_loss: 2.2998  time: 168s\n",
      "Epoch 5 - avg_train_loss: 2.5969  avg_val_loss: 2.2998  time: 168s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9531\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9531\n",
      "Epoch 5 - Save Best Score: 0.9531 Model\n",
      "Epoch 5 - Save Best Score: 0.9531 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/471] Elapsed 1m 11s (remain 559m 24s) Loss: 2.3230(2.3230) Grad: 11.9775  LR: 0.006768  \n",
      "Epoch: [6][100/471] Elapsed 1m 16s (remain 4m 41s) Loss: 2.2317(2.5648) Grad: 21.5412  LR: 0.006768  \n",
      "Epoch: [6][200/471] Elapsed 1m 22s (remain 1m 50s) Loss: 2.6487(2.5717) Grad: 32.0915  LR: 0.006768  \n",
      "Epoch: [6][300/471] Elapsed 1m 28s (remain 0m 49s) Loss: 2.3092(2.5695) Grad: 28.5659  LR: 0.006768  \n",
      "Epoch: [6][400/471] Elapsed 1m 34s (remain 0m 16s) Loss: 2.3165(2.5312) Grad: 6.7706  LR: 0.006768  \n",
      "Epoch: [6][470/471] Elapsed 1m 38s (remain 0m 0s) Loss: 2.3371(2.5150) Grad: 18.6403  LR: 0.006768  \n",
      "EVAL: [0/118] Elapsed 1m 7s (remain 132m 3s) Loss: 2.0047(2.0047) \n",
      "EVAL: [100/118] Elapsed 1m 13s (remain 0m 12s) Loss: 2.1580(2.2660) \n",
      "EVAL: [117/118] Elapsed 1m 13s (remain 0m 0s) Loss: 1.9402(2.2624) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 2.5150  avg_val_loss: 2.2624  time: 178s\n",
      "Epoch 6 - avg_train_loss: 2.5150  avg_val_loss: 2.2624  time: 178s\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.9410\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.9410\n",
      "Epoch 6 - Save Best Score: 0.9410 Model\n",
      "Epoch 6 - Save Best Score: 0.9410 Model\n",
      "========== fold: 1 result ==========\n",
      "========== fold: 1 result ==========\n",
      "Score (without expiratory phase): 0.9410\n",
      "Score (without expiratory phase): 0.9410\n",
      "========== fold: 2 training ==========\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 19s (remain 620m 40s) Loss: 41.0473(41.0473) Grad: 33.0008  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 25s (remain 5m 12s) Loss: 6.8188(15.4173) Grad: 11.0648  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 31s (remain 2m 2s) Loss: 4.6360(10.4532) Grad: 10.2503  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 37s (remain 0m 54s) Loss: 3.9819(8.4273) Grad: 48.5992  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 42s (remain 0m 17s) Loss: 4.0008(7.2917) Grad: 38.5118  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 46s (remain 0m 0s) Loss: 3.2554(6.7360) Grad: 19.2335  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 126m 57s) Loss: 4.5356(4.5356) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 3.8336(3.9290) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 3.4840(3.9363) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.7360  avg_val_loss: 3.9363  time: 182s\n",
      "Epoch 1 - avg_train_loss: 6.7360  avg_val_loss: 3.9363  time: 182s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.7061\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.7061\n",
      "Epoch 1 - Save Best Score: 1.7061 Model\n",
      "Epoch 1 - Save Best Score: 1.7061 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 15s (remain 594m 53s) Loss: 4.1829(4.1829) Grad: 50.8996  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 21s (remain 5m 0s) Loss: 3.8594(3.4260) Grad: 41.3475  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 28s (remain 1m 58s) Loss: 3.2167(3.3527) Grad: 14.2032  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 3.0953(3.2832) Grad: 51.8801  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 40s (remain 0m 17s) Loss: 3.8100(3.2457) Grad: 44.8668  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 44s (remain 0m 0s) Loss: 2.5118(3.2029) Grad: 23.3444  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 125m 13s) Loss: 3.1734(3.1734) \n",
      "EVAL: [100/118] Elapsed 1m 9s (remain 0m 11s) Loss: 2.7117(2.9090) \n",
      "EVAL: [117/118] Elapsed 1m 9s (remain 0m 0s) Loss: 2.7500(2.9246) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.2029  avg_val_loss: 2.9246  time: 178s\n",
      "Epoch 2 - avg_train_loss: 3.2029  avg_val_loss: 2.9246  time: 178s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.2181\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.2181\n",
      "Epoch 2 - Save Best Score: 1.2181 Model\n",
      "Epoch 2 - Save Best Score: 1.2181 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 16s (remain 597m 35s) Loss: 4.0427(4.0427) Grad: 42.7197  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 21s (remain 5m 0s) Loss: 3.1741(2.9313) Grad: 42.0011  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 27s (remain 1m 57s) Loss: 3.2898(3.2246) Grad: 22.5467  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 32s (remain 0m 52s) Loss: 3.2613(3.2007) Grad: 23.9255  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 38s (remain 0m 17s) Loss: 3.0302(3.1592) Grad: 8.8838  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 42s (remain 0m 0s) Loss: 2.6766(3.1083) Grad: 20.0963  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 125m 53s) Loss: 3.0867(3.0867) \n",
      "EVAL: [100/118] Elapsed 1m 9s (remain 0m 11s) Loss: 2.7378(2.9807) \n",
      "EVAL: [117/118] Elapsed 1m 9s (remain 0m 0s) Loss: 2.8257(2.9979) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 3.1083  avg_val_loss: 2.9979  time: 177s\n",
      "Epoch 3 - avg_train_loss: 3.1083  avg_val_loss: 2.9979  time: 177s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.2473\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.2473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 15s (remain 590m 20s) Loss: 3.1584(3.1584) Grad: 48.9188  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 21s (remain 4m 58s) Loss: 2.6208(2.8663) Grad: 19.8717  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 27s (remain 1m 58s) Loss: 2.4985(2.7982) Grad: 12.2650  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 2.7172(2.7954) Grad: 43.2347  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 40s (remain 0m 17s) Loss: 2.9629(2.7918) Grad: 23.5835  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 44s (remain 0m 0s) Loss: 2.6132(2.7697) Grad: 32.3932  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 5s (remain 127m 51s) Loss: 2.6028(2.6028) \n",
      "EVAL: [100/118] Elapsed 1m 10s (remain 0m 11s) Loss: 2.3184(2.4407) \n",
      "EVAL: [117/118] Elapsed 1m 10s (remain 0m 0s) Loss: 2.2066(2.4546) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.7697  avg_val_loss: 2.4546  time: 180s\n",
      "Epoch 4 - avg_train_loss: 2.7697  avg_val_loss: 2.4546  time: 180s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0067\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0067\n",
      "Epoch 4 - Save Best Score: 1.0067 Model\n",
      "Epoch 4 - Save Best Score: 1.0067 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 15s (remain 594m 18s) Loss: 2.4977(2.4977) Grad: 22.2954  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 21s (remain 4m 58s) Loss: 2.7132(2.6554) Grad: 12.8269  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 27s (remain 1m 56s) Loss: 2.4990(2.6440) Grad: 22.0746  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 32s (remain 0m 52s) Loss: 2.6639(2.6189) Grad: 21.7279  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 38s (remain 0m 17s) Loss: 2.5187(2.6015) Grad: 17.6389  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 41s (remain 0m 0s) Loss: 2.4308(2.5985) Grad: 10.2353  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 120m 36s) Loss: 2.5937(2.5937) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 2.2527(2.3456) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 2.1695(2.3584) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5985  avg_val_loss: 2.3584  time: 173s\n",
      "Epoch 5 - avg_train_loss: 2.5985  avg_val_loss: 2.3584  time: 173s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9865\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9865\n",
      "Epoch 5 - Save Best Score: 0.9865 Model\n",
      "Epoch 5 - Save Best Score: 0.9865 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/471] Elapsed 1m 11s (remain 563m 7s) Loss: 2.6773(2.6773) Grad: 20.7697  LR: 0.006768  \n",
      "Epoch: [6][100/471] Elapsed 1m 17s (remain 4m 43s) Loss: 2.7321(2.5449) Grad: 24.1578  LR: 0.006768  \n",
      "Epoch: [6][200/471] Elapsed 1m 22s (remain 1m 50s) Loss: 2.5154(2.5833) Grad: 11.1637  LR: 0.006768  \n",
      "Epoch: [6][300/471] Elapsed 1m 28s (remain 0m 49s) Loss: 2.3064(2.5562) Grad: 14.4709  LR: 0.006768  \n",
      "Epoch: [6][400/471] Elapsed 1m 33s (remain 0m 16s) Loss: 2.3592(2.5253) Grad: 19.0043  LR: 0.006768  \n",
      "Epoch: [6][470/471] Elapsed 1m 37s (remain 0m 0s) Loss: 2.4901(2.5115) Grad: 23.7037  LR: 0.006768  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 120m 41s) Loss: 2.5838(2.5838) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 2.2548(2.3260) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 2.1777(2.3376) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 2.5115  avg_val_loss: 2.3376  time: 168s\n",
      "Epoch 6 - avg_train_loss: 2.5115  avg_val_loss: 2.3376  time: 168s\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.9799\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.9799\n",
      "Epoch 6 - Save Best Score: 0.9799 Model\n",
      "Epoch 6 - Save Best Score: 0.9799 Model\n",
      "========== fold: 2 result ==========\n",
      "========== fold: 2 result ==========\n",
      "Score (without expiratory phase): 0.9799\n",
      "Score (without expiratory phase): 0.9799\n",
      "========== fold: 3 training ==========\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 14s (remain 584m 54s) Loss: 41.0599(41.0599) Grad: 33.3878  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 20s (remain 4m 54s) Loss: 6.0426(16.4843) Grad: 21.8668  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 25s (remain 1m 55s) Loss: 4.5628(10.9560) Grad: 10.4668  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 31s (remain 0m 51s) Loss: 4.3731(8.7517) Grad: 30.3035  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 36s (remain 0m 16s) Loss: 3.3967(7.5386) Grad: 16.6308  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 39s (remain 0m 0s) Loss: 4.5101(6.9644) Grad: 48.1732  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 1m 1s (remain 120m 9s) Loss: 3.5635(3.5635) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 3.0110(3.3254) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 3.5514(3.3128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.9644  avg_val_loss: 3.3128  time: 171s\n",
      "Epoch 1 - avg_train_loss: 6.9644  avg_val_loss: 3.3128  time: 171s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4195\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4195\n",
      "Epoch 1 - Save Best Score: 1.4195 Model\n",
      "Epoch 1 - Save Best Score: 1.4195 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 14s (remain 584m 30s) Loss: 3.6636(3.6636) Grad: 29.7363  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 21s (remain 4m 58s) Loss: 3.6053(3.4434) Grad: 40.3818  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 28s (remain 1m 58s) Loss: 3.3818(3.4045) Grad: 17.4583  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 3.1385(3.3190) Grad: 34.9597  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 41s (remain 0m 17s) Loss: 3.0620(3.2645) Grad: 25.0525  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 45s (remain 0m 0s) Loss: 2.9626(3.2387) Grad: 24.2415  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 11s (remain 138m 53s) Loss: 2.9087(2.9087) \n",
      "EVAL: [100/118] Elapsed 1m 16s (remain 0m 12s) Loss: 2.5472(2.7296) \n",
      "EVAL: [117/118] Elapsed 1m 17s (remain 0m 0s) Loss: 2.8141(2.7206) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.2387  avg_val_loss: 2.7206  time: 188s\n",
      "Epoch 2 - avg_train_loss: 3.2387  avg_val_loss: 2.7206  time: 188s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1170\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1170\n",
      "Epoch 2 - Save Best Score: 1.1170 Model\n",
      "Epoch 2 - Save Best Score: 1.1170 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 22s (remain 650m 0s) Loss: 3.0909(3.0909) Grad: 10.4896  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 29s (remain 5m 29s) Loss: 2.8140(2.9557) Grad: 12.2810  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 36s (remain 2m 9s) Loss: 2.7109(2.9128) Grad: 28.3992  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 42s (remain 0m 57s) Loss: 3.0889(2.8951) Grad: 30.3058  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 48s (remain 0m 18s) Loss: 2.5319(2.8734) Grad: 17.1744  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 51s (remain 0m 0s) Loss: 2.8178(2.8520) Grad: 22.4959  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 1m 7s (remain 131m 12s) Loss: 2.6954(2.6954) \n",
      "EVAL: [100/118] Elapsed 1m 12s (remain 0m 12s) Loss: 2.2840(2.4763) \n",
      "EVAL: [117/118] Elapsed 1m 12s (remain 0m 0s) Loss: 2.4640(2.4729) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.8520  avg_val_loss: 2.4729  time: 189s\n",
      "Epoch 3 - avg_train_loss: 2.8520  avg_val_loss: 2.4729  time: 189s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0389\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.0389\n",
      "Epoch 3 - Save Best Score: 1.0389 Model\n",
      "Epoch 3 - Save Best Score: 1.0389 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 16s (remain 599m 22s) Loss: 2.5034(2.5034) Grad: 11.9808  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 22s (remain 5m 2s) Loss: 2.8265(2.7593) Grad: 17.9008  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 28s (remain 1m 58s) Loss: 2.8731(2.7296) Grad: 11.8061  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 34s (remain 0m 53s) Loss: 2.8518(2.7072) Grad: 31.1295  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 40s (remain 0m 17s) Loss: 2.4766(2.6871) Grad: 9.1221  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 44s (remain 0m 0s) Loss: 2.6549(2.6763) Grad: 13.7201  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 4s (remain 125m 14s) Loss: 2.5157(2.5157) \n",
      "EVAL: [100/118] Elapsed 1m 8s (remain 0m 11s) Loss: 2.2130(2.3284) \n",
      "EVAL: [117/118] Elapsed 1m 9s (remain 0m 0s) Loss: 2.4452(2.3229) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.6763  avg_val_loss: 2.3229  time: 178s\n",
      "Epoch 4 - avg_train_loss: 2.6763  avg_val_loss: 2.3229  time: 178s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 0.9675\n",
      "Epoch 4 - MAE Score (without expiratory phase): 0.9675\n",
      "Epoch 4 - Save Best Score: 0.9675 Model\n",
      "Epoch 4 - Save Best Score: 0.9675 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 14s (remain 585m 59s) Loss: 2.5040(2.5040) Grad: 6.8518  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 20s (remain 4m 54s) Loss: 2.4583(2.5971) Grad: 19.2531  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 25s (remain 1m 55s) Loss: 2.7432(2.6029) Grad: 20.4749  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 31s (remain 0m 51s) Loss: 2.5140(2.5994) Grad: 18.8859  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 37s (remain 0m 16s) Loss: 2.3180(2.5930) Grad: 8.4046  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 40s (remain 0m 0s) Loss: 2.7153(2.5855) Grad: 16.3893  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 2s (remain 121m 59s) Loss: 2.2473(2.2473) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 2.1211(2.2082) \n",
      "EVAL: [117/118] Elapsed 1m 7s (remain 0m 0s) Loss: 2.3553(2.2067) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5855  avg_val_loss: 2.2067  time: 173s\n",
      "Epoch 5 - avg_train_loss: 2.5855  avg_val_loss: 2.2067  time: 173s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9074\n",
      "Epoch 5 - MAE Score (without expiratory phase): 0.9074\n",
      "Epoch 5 - Save Best Score: 0.9074 Model\n",
      "Epoch 5 - Save Best Score: 0.9074 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/471] Elapsed 1m 14s (remain 580m 15s) Loss: 1.9218(1.9218) Grad: 5.3817  LR: 0.006768  \n",
      "Epoch: [6][100/471] Elapsed 1m 19s (remain 4m 52s) Loss: 2.6046(2.6080) Grad: 30.2828  LR: 0.006768  \n",
      "Epoch: [6][200/471] Elapsed 1m 25s (remain 1m 54s) Loss: 2.6504(2.5775) Grad: 12.7483  LR: 0.006768  \n",
      "Epoch: [6][300/471] Elapsed 1m 30s (remain 0m 51s) Loss: 2.6852(2.5291) Grad: 9.7131  LR: 0.006768  \n",
      "Epoch: [6][400/471] Elapsed 1m 36s (remain 0m 16s) Loss: 2.4053(2.5077) Grad: 24.0765  LR: 0.006768  \n",
      "Epoch: [6][470/471] Elapsed 1m 39s (remain 0m 0s) Loss: 2.2088(2.5129) Grad: 5.7297  LR: 0.006768  \n",
      "EVAL: [0/118] Elapsed 1m 2s (remain 122m 21s) Loss: 2.5660(2.5660) \n",
      "EVAL: [100/118] Elapsed 1m 7s (remain 0m 11s) Loss: 2.1932(2.4645) \n",
      "EVAL: [117/118] Elapsed 1m 7s (remain 0m 0s) Loss: 2.8123(2.4660) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 2.5129  avg_val_loss: 2.4660  time: 172s\n",
      "Epoch 6 - avg_train_loss: 2.5129  avg_val_loss: 2.4660  time: 172s\n",
      "Epoch 6 - MAE Score (without expiratory phase): 1.0362\n",
      "Epoch 6 - MAE Score (without expiratory phase): 1.0362\n",
      "========== fold: 3 result ==========\n",
      "========== fold: 3 result ==========\n",
      "Score (without expiratory phase): 0.9074\n",
      "Score (without expiratory phase): 0.9074\n",
      "========== fold: 4 training ==========\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "Epoch: [1][0/471] Elapsed 1m 13s (remain 572m 34s) Loss: 42.8424(42.8424) Grad: 33.5078  LR: 0.007000  \n",
      "Epoch: [1][100/471] Elapsed 1m 18s (remain 4m 48s) Loss: 6.0007(15.6427) Grad: 16.4770  LR: 0.007000  \n",
      "Epoch: [1][200/471] Elapsed 1m 24s (remain 1m 53s) Loss: 4.5813(10.4803) Grad: 13.2719  LR: 0.007000  \n",
      "Epoch: [1][300/471] Elapsed 1m 29s (remain 0m 50s) Loss: 3.7041(8.4453) Grad: 14.0504  LR: 0.007000  \n",
      "Epoch: [1][400/471] Elapsed 1m 35s (remain 0m 16s) Loss: 3.8085(7.2820) Grad: 26.9375  LR: 0.007000  \n",
      "Epoch: [1][470/471] Elapsed 1m 38s (remain 0m 0s) Loss: 3.7057(6.7397) Grad: 30.3495  LR: 0.007000  \n",
      "EVAL: [0/118] Elapsed 0m 59s (remain 116m 47s) Loss: 3.1893(3.1893) \n",
      "EVAL: [100/118] Elapsed 1m 4s (remain 0m 10s) Loss: 3.6039(3.2894) \n",
      "EVAL: [117/118] Elapsed 1m 4s (remain 0m 0s) Loss: 3.1445(3.3013) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 6.7397  avg_val_loss: 3.3013  time: 167s\n",
      "Epoch 1 - avg_train_loss: 6.7397  avg_val_loss: 3.3013  time: 167s\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4062\n",
      "Epoch 1 - MAE Score (without expiratory phase): 1.4062\n",
      "Epoch 1 - Save Best Score: 1.4062 Model\n",
      "Epoch 1 - Save Best Score: 1.4062 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/471] Elapsed 1m 10s (remain 549m 2s) Loss: 3.3152(3.3152) Grad: 25.9525  LR: 0.006986  \n",
      "Epoch: [2][100/471] Elapsed 1m 15s (remain 4m 36s) Loss: 3.9369(3.4159) Grad: 28.8377  LR: 0.006986  \n",
      "Epoch: [2][200/471] Elapsed 1m 20s (remain 1m 48s) Loss: 3.2237(3.3481) Grad: 18.6038  LR: 0.006986  \n",
      "Epoch: [2][300/471] Elapsed 1m 25s (remain 0m 48s) Loss: 3.1329(3.3178) Grad: 21.7290  LR: 0.006986  \n",
      "Epoch: [2][400/471] Elapsed 1m 30s (remain 0m 15s) Loss: 2.8941(3.2674) Grad: 8.8727  LR: 0.006986  \n",
      "Epoch: [2][470/471] Elapsed 1m 33s (remain 0m 0s) Loss: 2.7167(3.2228) Grad: 5.6803  LR: 0.006986  \n",
      "EVAL: [0/118] Elapsed 1m 0s (remain 118m 4s) Loss: 2.8148(2.8148) \n",
      "EVAL: [100/118] Elapsed 1m 4s (remain 0m 10s) Loss: 2.9056(2.7645) \n",
      "EVAL: [117/118] Elapsed 1m 4s (remain 0m 0s) Loss: 2.6163(2.7692) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 3.2228  avg_val_loss: 2.7692  time: 163s\n",
      "Epoch 2 - avg_train_loss: 3.2228  avg_val_loss: 2.7692  time: 163s\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1666\n",
      "Epoch 2 - MAE Score (without expiratory phase): 1.1666\n",
      "Epoch 2 - Save Best Score: 1.1666 Model\n",
      "Epoch 2 - Save Best Score: 1.1666 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/471] Elapsed 1m 9s (remain 546m 34s) Loss: 2.9012(2.9012) Grad: 15.0871  LR: 0.006952  \n",
      "Epoch: [3][100/471] Elapsed 1m 15s (remain 4m 34s) Loss: 2.7683(2.9970) Grad: 13.6528  LR: 0.006952  \n",
      "Epoch: [3][200/471] Elapsed 1m 20s (remain 1m 47s) Loss: 2.6062(2.9607) Grad: 8.6698  LR: 0.006952  \n",
      "Epoch: [3][300/471] Elapsed 1m 25s (remain 0m 48s) Loss: 3.0208(2.9255) Grad: 14.5435  LR: 0.006952  \n",
      "Epoch: [3][400/471] Elapsed 1m 30s (remain 0m 15s) Loss: 3.5294(2.8952) Grad: 39.6786  LR: 0.006952  \n",
      "Epoch: [3][470/471] Elapsed 1m 34s (remain 0m 0s) Loss: 3.0174(2.8924) Grad: 12.8640  LR: 0.006952  \n",
      "EVAL: [0/118] Elapsed 0m 59s (remain 116m 10s) Loss: 2.8143(2.8143) \n",
      "EVAL: [100/118] Elapsed 1m 4s (remain 0m 10s) Loss: 2.8722(2.8865) \n",
      "EVAL: [117/118] Elapsed 1m 4s (remain 0m 0s) Loss: 2.8256(2.8897) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 2.8924  avg_val_loss: 2.8897  time: 163s\n",
      "Epoch 3 - avg_train_loss: 2.8924  avg_val_loss: 2.8897  time: 163s\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.2062\n",
      "Epoch 3 - MAE Score (without expiratory phase): 1.2062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/471] Elapsed 1m 9s (remain 548m 14s) Loss: 3.0835(3.0835) Grad: 23.7358  LR: 0.006904  \n",
      "Epoch: [4][100/471] Elapsed 1m 15s (remain 4m 35s) Loss: 2.9912(2.7843) Grad: 28.9607  LR: 0.006904  \n",
      "Epoch: [4][200/471] Elapsed 1m 20s (remain 1m 47s) Loss: 2.7598(2.7483) Grad: 17.1844  LR: 0.006904  \n",
      "Epoch: [4][300/471] Elapsed 1m 25s (remain 0m 48s) Loss: 2.5728(2.7243) Grad: 10.5061  LR: 0.006904  \n",
      "Epoch: [4][400/471] Elapsed 1m 30s (remain 0m 15s) Loss: 2.5852(2.7099) Grad: 9.1929  LR: 0.006904  \n",
      "Epoch: [4][470/471] Elapsed 1m 33s (remain 0m 0s) Loss: 2.7166(2.6999) Grad: 20.9535  LR: 0.006904  \n",
      "EVAL: [0/118] Elapsed 1m 0s (remain 117m 34s) Loss: 2.4278(2.4278) \n",
      "EVAL: [100/118] Elapsed 1m 6s (remain 0m 11s) Loss: 2.6251(2.4322) \n",
      "EVAL: [117/118] Elapsed 1m 6s (remain 0m 0s) Loss: 2.4249(2.4372) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 2.6999  avg_val_loss: 2.4372  time: 165s\n",
      "Epoch 4 - avg_train_loss: 2.6999  avg_val_loss: 2.4372  time: 165s\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0279\n",
      "Epoch 4 - MAE Score (without expiratory phase): 1.0279\n",
      "Epoch 4 - Save Best Score: 1.0279 Model\n",
      "Epoch 4 - Save Best Score: 1.0279 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/471] Elapsed 1m 27s (remain 687m 23s) Loss: 2.5359(2.5359) Grad: 22.2780  LR: 0.006842  \n",
      "Epoch: [5][100/471] Elapsed 1m 34s (remain 5m 45s) Loss: 2.5359(2.6361) Grad: 9.0610  LR: 0.006842  \n",
      "Epoch: [5][200/471] Elapsed 1m 40s (remain 2m 15s) Loss: 2.5971(2.5881) Grad: 10.0137  LR: 0.006842  \n",
      "Epoch: [5][300/471] Elapsed 1m 47s (remain 1m 0s) Loss: 2.6131(2.5799) Grad: 17.7621  LR: 0.006842  \n",
      "Epoch: [5][400/471] Elapsed 1m 53s (remain 0m 19s) Loss: 2.3780(2.5749) Grad: 10.3528  LR: 0.006842  \n",
      "Epoch: [5][470/471] Elapsed 1m 57s (remain 0m 0s) Loss: 2.7017(2.5691) Grad: 22.6863  LR: 0.006842  \n",
      "EVAL: [0/118] Elapsed 1m 9s (remain 135m 8s) Loss: 2.5483(2.5483) \n",
      "EVAL: [100/118] Elapsed 1m 14s (remain 0m 12s) Loss: 2.5779(2.4478) \n",
      "EVAL: [117/118] Elapsed 1m 14s (remain 0m 0s) Loss: 2.6995(2.4532) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 2.5691  avg_val_loss: 2.4532  time: 198s\n",
      "Epoch 5 - avg_train_loss: 2.5691  avg_val_loss: 2.4532  time: 198s\n",
      "Epoch 5 - MAE Score (without expiratory phase): 1.0237\n",
      "Epoch 5 - MAE Score (without expiratory phase): 1.0237\n",
      "Epoch 5 - Save Best Score: 1.0237 Model\n",
      "Epoch 5 - Save Best Score: 1.0237 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/471] Elapsed 1m 21s (remain 639m 29s) Loss: 2.5599(2.5599) Grad: 21.6033  LR: 0.006768  \n",
      "Epoch: [6][100/471] Elapsed 1m 27s (remain 5m 21s) Loss: 2.5266(2.5139) Grad: 23.9453  LR: 0.006768  \n",
      "Epoch: [6][200/471] Elapsed 1m 33s (remain 2m 6s) Loss: 2.1349(2.5035) Grad: 17.1179  LR: 0.006768  \n",
      "Epoch: [6][300/471] Elapsed 1m 40s (remain 0m 56s) Loss: 2.2957(2.4793) Grad: 12.0628  LR: 0.006768  \n",
      "Epoch: [6][400/471] Elapsed 1m 47s (remain 0m 18s) Loss: 2.5701(2.4853) Grad: 12.7700  LR: 0.006768  \n",
      "Epoch: [6][470/471] Elapsed 1m 51s (remain 0m 0s) Loss: 2.5966(2.4741) Grad: 14.9475  LR: 0.006768  \n",
      "EVAL: [0/118] Elapsed 1m 9s (remain 135m 16s) Loss: 2.2081(2.2081) \n",
      "EVAL: [100/118] Elapsed 1m 14s (remain 0m 12s) Loss: 2.3050(2.2058) \n",
      "EVAL: [117/118] Elapsed 1m 15s (remain 0m 0s) Loss: 2.2761(2.2048) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 2.4741  avg_val_loss: 2.2048  time: 191s\n",
      "Epoch 6 - avg_train_loss: 2.4741  avg_val_loss: 2.2048  time: 191s\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.9111\n",
      "Epoch 6 - MAE Score (without expiratory phase): 0.9111\n",
      "Epoch 6 - Save Best Score: 0.9111 Model\n",
      "Epoch 6 - Save Best Score: 0.9111 Model\n",
      "========== fold: 4 result ==========\n",
      "========== fold: 4 result ==========\n",
      "Score (without expiratory phase): 0.9111\n",
      "Score (without expiratory phase): 0.9111\n",
      "========== CV ==========\n",
      "========== CV ==========\n",
      "Score (without expiratory phase): 0.9314\n",
      "Score (without expiratory phase): 0.9314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomModel:\n\tMissing key(s) in state_dict: \"r_emb.weight\", \"c_emb.weight\", \"seq_emb.0.weight\", \"seq_emb.0.bias\", \"seq_emb.1.weight\", \"seq_emb.1.bias\", \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"lstm.weight_ih_l0_reverse\", \"lstm.weight_hh_l0_reverse\", \"lstm.bias_ih_l0_reverse\", \"lstm.bias_hh_l0_reverse\", \"head.0.weight\", \"head.0.bias\", \"head.1.weight\", \"head.1.bias\", \"head.4.weight\", \"head.4.bias\". \n\tUnexpected key(s) in state_dict: \"module.r_emb.weight\", \"module.c_emb.weight\", \"module.seq_emb.0.weight\", \"module.seq_emb.0.bias\", \"module.seq_emb.1.weight\", \"module.seq_emb.1.bias\", \"module.lstm.weight_ih_l0\", \"module.lstm.weight_hh_l0\", \"module.lstm.bias_ih_l0\", \"module.lstm.bias_hh_l0\", \"module.lstm.weight_ih_l0_reverse\", \"module.lstm.weight_hh_l0_reverse\", \"module.lstm.bias_ih_l0_reverse\", \"module.lstm.bias_hh_l0_reverse\", \"module.head.0.weight\", \"module.head.0.bias\", \"module.head.1.weight\", \"module.head.1.bias\", \"module.head.4.weight\", \"module.head.4.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-690d7c1996d9>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34mf\"fold{fold}_best.pth\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'fold{fold}'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1224\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomModel:\n\tMissing key(s) in state_dict: \"r_emb.weight\", \"c_emb.weight\", \"seq_emb.0.weight\", \"seq_emb.0.bias\", \"seq_emb.1.weight\", \"seq_emb.1.bias\", \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"lstm.weight_ih_l0_reverse\", \"lstm.weight_hh_l0_reverse\", \"lstm.bias_ih_l0_reverse\", \"lstm.bias_hh_l0_reverse\", \"head.0.weight\", \"head.0.bias\", \"head.1.weight\", \"head.1.bias\", \"head.4.weight\", \"head.4.bias\". \n\tUnexpected key(s) in state_dict: \"module.r_emb.weight\", \"module.c_emb.weight\", \"module.seq_emb.0.weight\", \"module.seq_emb.0.bias\", \"module.seq_emb.1.weight\", \"module.seq_emb.1.bias\", \"module.lstm.weight_ih_l0\", \"module.lstm.weight_hh_l0\", \"module.lstm.bias_ih_l0\", \"module.lstm.bias_hh_l0\", \"module.lstm.weight_ih_l0_reverse\", \"module.lstm.weight_hh_l0_reverse\", \"module.lstm.bias_ih_l0_reverse\", \"module.lstm.bias_hh_l0_reverse\", \"module.head.0.weight\", \"module.head.0.bias\", \"module.head.1.weight\", \"module.head.1.bias\", \"module.head.4.weight\", \"module.head.4.bias\". "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e33556e2a34c24870609d7d39692f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=197.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b807a857fd44af83441f1a4479c54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=197.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb23736aba04a4f8a76bdbba4f4e809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=197.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d959002846aa46a38e34e41febbd953d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=197.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "init LSTM(64, 64, batch_first=True, dropout=0.2, bidirectional=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa2f10865e347fcbc23fe424d4f9c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=197.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n",
    "for fold in CFG.trn_fold:\n",
    "    model = CustomModel(CFG)\n",
    "    path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n",
    "    state = torch.load(path, map_location=torch.device('cpu'))\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(state['model'])\n",
    "    predictions = inference_fn(test_loader, model, device)\n",
    "    test[f'fold{fold}'] = predictions\n",
    "    del state, predictions; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "# submission\n",
    "test['pressure'] = test[[f'fold{fold}' for fold in range(CFG.n_fold)]].mean(1)\n",
    "test[['id', 'pressure']+[f'fold{fold}' for fold in range(CFG.n_fold)]].to_csv(OUTPUT_DIR+'raw_submission.csv', index=False)\n",
    "test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original saved file with DataParallel\n",
    "state_dict = torch.load('myfile.pth.tar')\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "model.load_state_dict(new_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
